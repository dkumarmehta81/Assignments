{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.backend import function\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,f1_score\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.keras.layers import Input, Multiply\nfrom tensorflow.keras.applications import InceptionV3,MobileNet\nfrom sklearn.utils import resample\nimport keras as K","metadata":{"execution":{"iopub.status.busy":"2023-10-12T13:13:07.309505Z","iopub.execute_input":"2023-10-12T13:13:07.309757Z","iopub.status.idle":"2023-10-12T13:13:16.743847Z","shell.execute_reply.started":"2023-10-12T13:13:07.309732Z","shell.execute_reply":"2023-10-12T13:13:16.742838Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set a random seed for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T13:13:16.745597Z","iopub.execute_input":"2023-10-12T13:13:16.746204Z","iopub.status.idle":"2023-10-12T13:13:16.751336Z","shell.execute_reply.started":"2023-10-12T13:13:16.746170Z","shell.execute_reply":"2023-10-12T13:13:16.750425Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Local Attention Module Declaration","metadata":{}},{"cell_type":"code","source":"\ndef attention_module(input_tensor, kernel_size=3):\n    # Local attention module implementation\n    x = Conv2D(filters=1, kernel_size=(1, 1), activation='sigmoid')(input_tensor)\n    x = Multiply()([input_tensor, x])\n\n    # Apply local attention by convolving with a kernel centered on each pixel\n    local_attention = Conv2D(filters=1, kernel_size=kernel_size, padding='same', activation='sigmoid')(x)\n    x = Multiply()([x, local_attention])\n    \n    return x\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T13:13:16.752759Z","iopub.execute_input":"2023-10-12T13:13:16.753390Z","iopub.status.idle":"2023-10-12T13:13:16.765215Z","shell.execute_reply.started":"2023-10-12T13:13:16.753354Z","shell.execute_reply":"2023-10-12T13:13:16.764278Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def spatial_attention(input_feature):\n\tkernel_size = 7\n\t\n\tif K.image_data_format() == \"channels_first\":\n\t\tchannel = input_feature.shape[1]\n\t\tcbam_feature = Permute((2,3,1))(input_feature)\n\telse:\n\t\tchannel = input_feature.shape[-1]\n\t\tcbam_feature = input_feature\n\t\n\tavg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n\tassert avg_pool.shape[-1] == 1\n\tmax_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n\tassert max_pool.shape[-1] == 1\n\tconcat = Concatenate(axis=3)([avg_pool, max_pool])\n\tassert concat.shape[-1] == 2\n\tcbam_feature = Conv2D(filters = 1,\n\t\t\t\t\tkernel_size=kernel_size,\n\t\t\t\t\tstrides=1,\n\t\t\t\t\tpadding='same',\n\t\t\t\t\tactivation='sigmoid',\n\t\t\t\t\tkernel_initializer='he_normal',\n\t\t\t\t\tuse_bias=False)(concat)\t\n\tassert cbam_feature.shape[-1] == 1\n\t\n\tif K.image_data_format() == \"channels_first\":\n\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n\t\t\n\treturn multiply([input_feature, cbam_feature])\n\t\t","metadata":{"execution":{"iopub.status.busy":"2023-10-12T13:13:16.767684Z","iopub.execute_input":"2023-10-12T13:13:16.768558Z","iopub.status.idle":"2023-10-12T13:13:16.777709Z","shell.execute_reply.started":"2023-10-12T13:13:16.768528Z","shell.execute_reply":"2023-10-12T13:13:16.776566Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Local Mobile Net Model Fine Tuning","metadata":{}},{"cell_type":"code","source":"def create_Local_attention_augmented_mobilenet(input_shape, num_classes,attention=True):\n    # Load MobileNet base model without top layer\n    base_model = tf.keras.applications.MobileNet(include_top=False, weights='imagenet', input_shape=input_shape)\n    base_model.trainable = False\n\n    # Attention augmented MobileNet architecture\n    input_tensor = Input(shape=input_shape)\n    x = base_model(input_tensor)\n    if(attention):\n        x = spatial_attention(x)\n\n    # Add classification layers\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    output_tensor = Dense(num_classes, activation='sigmoid')(x)\n\n    # Create the model\n    model = Model(inputs=input_tensor, outputs=output_tensor)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-10-12T13:13:16.779013Z","iopub.execute_input":"2023-10-12T13:13:16.779936Z","iopub.status.idle":"2023-10-12T13:13:16.792612Z","shell.execute_reply.started":"2023-10-12T13:13:16.779905Z","shell.execute_reply":"2023-10-12T13:13:16.791599Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Local InceptionV3 Model Fine Tuning","metadata":{}},{"cell_type":"code","source":"def create_Local_Inceptionv3_model(input_shape, num_classes,attention=True):\n    \n    # Load Inception-v3 base model without top layer\n    base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=input_shape)\n    base_model.trainable = False\n\n    # Attention augmented Inception-v3 architecture\n    input_tensor = Input(shape=input_shape)\n    x = base_model(input_tensor)\n    if(attention):\n        x = Add()([spatial_attention(x),x])\n\n    # Add classification layers\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    output_tensor = Dense(num_classes, activation='sigmoid')(x)\n\n    # Create the model\n    model = Model(inputs=input_tensor, outputs=output_tensor)\n    # Freeze the weights of the base model (optional, can be skipped if you want to fine-tune later)\n    #for layer in base_model.layers:\n    #    layer.trainable = False\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-10-12T13:13:16.793858Z","iopub.execute_input":"2023-10-12T13:13:16.794670Z","iopub.status.idle":"2023-10-12T13:13:16.804676Z","shell.execute_reply.started":"2023-10-12T13:13:16.794638Z","shell.execute_reply":"2023-10-12T13:13:16.803651Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import classification_report, balanced_accuracy_score, average_precision_score, confusion_matrix\n\ndef evaluate_classification(y_true, y_pred, average='macro'):\n    \"\"\"\n    Evaluate the classification performance and calculate micro-average, balanced accuracy, and average precision.\n\n    Parameters:\n        y_true (numpy array or list): True labels.\n        y_pred (numpy array or list): Predicted labels.\n        average (str, optional): The averaging strategy to use for average precision.\n                                 Possible values are 'macro', 'micro', 'weighted', and None.\n                                 Default is 'macro'.\n\n    Returns:\n        report (str): The classification report as a string.\n        balanced_acc (float): The balanced accuracy.\n        avg_precision (float): The average precision.\n        micro_avg_precision (float): The micro-average precision.\n        micro_avg_recall (float): The micro-average recall.\n        micro_avg_f1_score (float): The micro-average F1-score.\n    \"\"\"\n    report = classification_report(y_true, y_pred, output_dict=True, zero_division=1)\n    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n    avg_precision = average_precision_score(y_true, y_pred, average=average)\n\n    # Calculate micro-average precision and recall using confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    tp_sum = np.sum(np.diag(cm))\n    pred_sum = np.sum(cm, axis=0)\n    true_sum = np.sum(cm, axis=1)\n    micro_avg_precision = tp_sum / pred_sum.sum()\n    micro_avg_recall = tp_sum / true_sum.sum()\n    micro_avg_f1_score = 2 * (micro_avg_precision * micro_avg_recall) / (micro_avg_precision + micro_avg_recall)\n\n    return report, balanced_acc, avg_precision, micro_avg_precision, micro_avg_recall, micro_avg_f1_score\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T13:13:16.805840Z","iopub.execute_input":"2023-10-12T13:13:16.806651Z","iopub.status.idle":"2023-10-12T13:13:16.818870Z","shell.execute_reply.started":"2023-10-12T13:13:16.806621Z","shell.execute_reply":"2023-10-12T13:13:16.817812Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# constant Declaration","metadata":{}},{"cell_type":"code","source":"# 1. Load and split the dataset\ntrain_data_dir = '/kaggle/input/cataract-image-dataset/processed_images/train'\n#validation_data_dir = 'd:/chaman/cataract/test'\ninput_shape = (224, 224)\nbatch_size = 32\nnum_classes=2\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T13:13:16.820096Z","iopub.execute_input":"2023-10-12T13:13:16.821086Z","iopub.status.idle":"2023-10-12T13:13:16.832929Z","shell.execute_reply.started":"2023-10-12T13:13:16.821031Z","shell.execute_reply":"2023-10-12T13:13:16.832042Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Traing and Validation + Data Augmentation Module","metadata":{}},{"cell_type":"code","source":"# 2. Preprocess the images\ntrain_datagen = ImageDataGenerator(\n    rescale=1.0/255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.2  # 20% validation split\n)\n\n#validation_datagen = ImageDataGenerator(rescale=1.0/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=input_shape,\n    batch_size=batch_size,\n    class_mode='categorical',  # Updated to 'categorical'\n    subset=\"training\"\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=input_shape,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'  # Subset for validation data\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T13:13:16.834394Z","iopub.execute_input":"2023-10-12T13:13:16.834738Z","iopub.status.idle":"2023-10-12T13:13:16.902129Z","shell.execute_reply.started":"2023-10-12T13:13:16.834661Z","shell.execute_reply":"2023-10-12T13:13:16.901345Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Found 393 images belonging to 2 classes.\nFound 98 images belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Local MobileNet Model Compilation","metadata":{}},{"cell_type":"code","source":"from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, multiply, Permute, Concatenate, Conv2D, Add, Activation, Lambda\nfrom keras import backend as K\nfrom keras.activations import sigmoid\nMobileNetModel = create_Local_attention_augmented_mobilenet(input_shape + (3,), num_classes)\n\n# 4. Compile the model\nMobileNetModel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T13:13:16.904527Z","iopub.execute_input":"2023-10-12T13:13:16.905425Z","iopub.status.idle":"2023-10-12T13:13:21.389210Z","shell.execute_reply.started":"2023-10-12T13:13:16.905392Z","shell.execute_reply":"2023-10-12T13:13:21.388287Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n17225924/17225924 [==============================] - 0s 0us/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Local  InceptionV3 Model Compilation","metadata":{}},{"cell_type":"code","source":"InceptionV3Model = create_Local_Inceptionv3_model(input_shape + (3,), num_classes)\n\nInceptionV3Model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T13:13:21.390955Z","iopub.execute_input":"2023-10-12T13:13:21.391486Z","iopub.status.idle":"2023-10-12T13:13:25.001635Z","shell.execute_reply.started":"2023-10-12T13:13:21.391455Z","shell.execute_reply":"2023-10-12T13:13:25.000710Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n87910968/87910968 [==============================] - 0s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, EarlyStopping\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',  # Monitor validation loss\n    patience=40,          # Number of epochs with no improvement to wait\n    restore_best_weights=True,  # Restore the best model weights when stopping\n\n)\ncp_callback1 = tf.keras.callbacks.ModelCheckpoint('./fpseweights.h5', save_weights_only=True, verbose=1, save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T13:13:25.003063Z","iopub.execute_input":"2023-10-12T13:13:25.003407Z","iopub.status.idle":"2023-10-12T13:13:25.008946Z","shell.execute_reply.started":"2023-10-12T13:13:25.003373Z","shell.execute_reply":"2023-10-12T13:13:25.008059Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"cp_callback2 = tf.keras.callbacks.ModelCheckpoint('./fpseweights1.h5', save_weights_only=True, verbose=1, save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T13:13:25.011176Z","iopub.execute_input":"2023-10-12T13:13:25.011854Z","iopub.status.idle":"2023-10-12T13:13:25.020708Z","shell.execute_reply.started":"2023-10-12T13:13:25.011795Z","shell.execute_reply":"2023-10-12T13:13:25.019612Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"cp_callback3 = tf.keras.callbacks.ModelCheckpoint('./fpseweights2.h5', save_weights_only=True, verbose=1, save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T13:13:25.021881Z","iopub.execute_input":"2023-10-12T13:13:25.022764Z","iopub.status.idle":"2023-10-12T13:13:25.032170Z","shell.execute_reply.started":"2023-10-12T13:13:25.022683Z","shell.execute_reply":"2023-10-12T13:13:25.031232Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Local MobileNet Model Training","metadata":{}},{"cell_type":"code","source":"# 5. Train the model\nepochs = 100\n\n# Create a directory to save the best model weights during training\nos.makedirs('models', exist_ok=True)\nmodel_checkpoint = ModelCheckpoint('models/best_localmobilnetmodel1.h5', save_best_only=True, save_weights_only=True)\n\nhistory = MobileNetModel.fit(\n    train_generator,\n    steps_per_epoch=train_generator.n // train_generator.batch_size,\n    epochs=epochs,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.n // validation_generator.batch_size,\n    callbacks=[model_checkpoint,cp_callback1]\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T09:35:14.488043Z","iopub.execute_input":"2023-10-12T09:35:14.488361Z","iopub.status.idle":"2023-10-12T10:09:12.832822Z","shell.execute_reply.started":"2023-10-12T09:35:14.488331Z","shell.execute_reply":"2023-10-12T10:09:12.831924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Local InceptionV3 Model Training","metadata":{}},{"cell_type":"code","source":"# Create a directory to save the best model weights during training\nos.makedirs('models', exist_ok=True)\nmodel_checkpoint = ModelCheckpoint('models/best_LocalInceptionModel1.h5', save_best_only=True, save_weights_only=True)\n\nInceptionV3history = InceptionV3Model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.n // train_generator.batch_size,\n    epochs=epochs,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.n // validation_generator.batch_size,\n    callbacks=[model_checkpoint,cp_callback2]\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:09:12.834677Z","iopub.execute_input":"2023-10-12T10:09:12.835042Z","iopub.status.idle":"2023-10-12T10:45:47.182638Z","shell.execute_reply.started":"2023-10-12T10:09:12.835009Z","shell.execute_reply":"2023-10-12T10:45:47.181553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MobileNetModel.save('LAAMD1.h5')\nInceptionV3Model.save('LAAIV3D1.h5')","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:45:47.184785Z","iopub.execute_input":"2023-10-12T10:45:47.185235Z","iopub.status.idle":"2023-10-12T10:45:47.864640Z","shell.execute_reply.started":"2023-10-12T10:45:47.185184Z","shell.execute_reply":"2023-10-12T10:45:47.863712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Local MobileNet Model Validation Evaluation","metadata":{}},{"cell_type":"code","source":"# 6. Evaluate the model\nMobileNetEvaluation = MobileNetModel.evaluate(validation_generator)\nprint(\"Validation Accuracy: {:.2f}%\".format(MobileNetEvaluation[1] * 100))","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:45:47.866037Z","iopub.execute_input":"2023-10-12T10:45:47.866363Z","iopub.status.idle":"2023-10-12T10:45:51.075008Z","shell.execute_reply.started":"2023-10-12T10:45:47.866334Z","shell.execute_reply":"2023-10-12T10:45:51.073806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Local InceptionV3 Model Validation Evaluation","metadata":{}},{"cell_type":"code","source":"# 6. Evaluate the model\nInceptionV3Evaluation = InceptionV3Model.evaluate(validation_generator)\nprint(\"Validation Accuracy: {:.2f}%\".format(InceptionV3Evaluation[1] * 100))","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:45:51.080498Z","iopub.execute_input":"2023-10-12T10:45:51.080823Z","iopub.status.idle":"2023-10-12T10:45:54.980582Z","shell.execute_reply.started":"2023-10-12T10:45:51.080787Z","shell.execute_reply":"2023-10-12T10:45:54.979564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Common TestGenerator Declaration Module","metadata":{}},{"cell_type":"code","source":"# Test data directory\ntest_data_dir = '/kaggle/input/cataract-image-dataset/processed_images/test'\ninput_shape = (224, 224)\nbatch_size = 32\n\n# Preprocess the test images\ntest_datagen = ImageDataGenerator(rescale=1.0/255)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_data_dir,\n    target_size=input_shape,\n    batch_size=batch_size,\n    class_mode='categorical',  # Set to 'categorical' if you used the updated code\n    shuffle=False\n)\n\n# Load the saved model\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# # Local MobileNet Model Test Evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluate the model on the test set\nMobileNetTestEvaluation = MobileNetModel.evaluate_generator(test_generator)\nprint(\"Test Accuracy: {:.2f}%\".format(MobileNetTestEvaluation[1] * 100))","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:50:36.188161Z","iopub.execute_input":"2023-10-12T10:50:36.189192Z","iopub.status.idle":"2023-10-12T10:50:42.803633Z","shell.execute_reply.started":"2023-10-12T10:50:36.189149Z","shell.execute_reply":"2023-10-12T10:50:42.802668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Local InceptionV3 Model Test Evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluate the model on the test set\nInceptionV3TestEvaluation = InceptionV3Model.evaluate_generator(test_generator)\nprint(\"Test Accuracy: {:.2f}%\".format(InceptionV3TestEvaluation[1] * 100))","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:50:48.934097Z","iopub.execute_input":"2023-10-12T10:50:48.934452Z","iopub.status.idle":"2023-10-12T10:50:55.039710Z","shell.execute_reply.started":"2023-10-12T10:50:48.934425Z","shell.execute_reply":"2023-10-12T10:50:55.038735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Common Classification Report Module","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Load the saved model\n#model = tf.keras.models.load_model('models/final_model.h5')  # Update with the correct path\n\n# Get the true labels for the test set\ntrue_labels = test_generator.classes\n\n# Compute and print the classification report\nclass_names = list(test_generator.class_indices.keys())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Local MobileNet Model Classification Report","metadata":{}},{"cell_type":"code","source":"# Generate predictions for the test set\nMobileNetPredictions = MobileNetModel.predict_generator(test_generator)\nMobileNetPredicted_labels = np.argmax(MobileNetPredictions, axis=1)\nMobileNetReport = classification_report(true_labels, MobileNetPredicted_labels, target_names=class_names)\nprint(MobileNetReport)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:51:04.355093Z","iopub.execute_input":"2023-10-12T10:51:04.355417Z","iopub.status.idle":"2023-10-12T10:51:09.951815Z","shell.execute_reply.started":"2023-10-12T10:51:04.355391Z","shell.execute_reply":"2023-10-12T10:51:09.950721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import classification_report, balanced_accuracy_score, average_precision_score, confusion_matrix\n\ndef evaluate_classification(y_true, y_pred, average='macro'):\n    \"\"\"\n    Evaluate the classification performance and calculate micro-average, balanced accuracy, and average precision.\n\n    Parameters:\n        y_true (numpy array or list): True labels.\n        y_pred (numpy array or list): Predicted labels.\n        average (str, optional): The averaging strategy to use for average precision.\n                                 Possible values are 'macro', 'micro', 'weighted', and None.\n                                 Default is 'macro'.\n\n    Returns:\n        report (str): The classification report as a string.\n        balanced_acc (float): The balanced accuracy.\n        avg_precision (float): The average precision.\n        micro_avg_precision (float): The micro-average precision.\n        micro_avg_recall (float): The micro-average recall.\n        micro_avg_f1_score (float): The micro-average F1-score.\n    \"\"\"\n    report = classification_report(y_true, y_pred, output_dict=True, zero_division=1)\n    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n    avg_precision = average_precision_score(y_true, y_pred, average=average)\n\n    # Calculate micro-average precision and recall using confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    tp_sum = np.sum(np.diag(cm))\n    pred_sum = np.sum(cm, axis=0)\n    true_sum = np.sum(cm, axis=1)\n    micro_avg_precision = tp_sum / pred_sum.sum()\n    micro_avg_recall = tp_sum / true_sum.sum()\n    micro_avg_f1_score = 2 * (micro_avg_precision * micro_avg_recall) / (micro_avg_precision + micro_avg_recall)\n\n    return report, balanced_acc, avg_precision, micro_avg_precision, micro_avg_recall, micro_avg_f1_score\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:51:19.366487Z","iopub.execute_input":"2023-10-12T10:51:19.367487Z","iopub.status.idle":"2023-10-12T10:51:19.375157Z","shell.execute_reply.started":"2023-10-12T10:51:19.367446Z","shell.execute_reply":"2023-10-12T10:51:19.374024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report, balanced_acc, avg_precision, micro_avg_precision, micro_avg_recall, micro_avg_f1_score = evaluate_classification(true_labels,  MobileNetPredicted_labels)\n\n# Print the classification report and additional metrics\n#print(\"Classification Report:\\n\", report)\nprint(\"Micro-average F1-score:\", micro_avg_f1_score)\nprint(\"Balanced Accuracy:\", balanced_acc)\nprint(\"Average Precision:\", avg_precision)\nprint(\"Micro-average Precision:\", micro_avg_precision)\nprint(\"Micro-average Recall:\", micro_avg_recall)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:51:21.548224Z","iopub.execute_input":"2023-10-12T10:51:21.548548Z","iopub.status.idle":"2023-10-12T10:51:21.566958Z","shell.execute_reply.started":"2023-10-12T10:51:21.548522Z","shell.execute_reply":"2023-10-12T10:51:21.566001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"import numpy as np\nfrom sklearn.metrics import roc_auc_score, balanced_accuracy_score, average_precision_score, f1_score\n\nnum_bootstrap_samples = 5000\nbootstrap_scores = []\n\nfor _ in range(num_bootstrap_samples):\n    # Generate a random bootstrap sample from the predictions with replacement\n    bootstrap_indices = np.random.choice(len(true_labels), len(true_labels), replace=True)\n    bootstrap_sample = MobileNetPredicted_labels[bootstrap_indices]\n\n    # Calculate evaluation metrics for the bootstrap sample\n    bootstrap_auc = roc_auc_score(true_labels[bootstrap_indices], bootstrap_sample)\n    bootstrap_balanced_accuracy = balanced_accuracy_score(true_labels[bootstrap_indices], bootstrap_sample)\n    bootstrap_avg_precision = average_precision_score(true_labels[bootstrap_indices], bootstrap_sample)\n    bootstrap_f1_score = f1_score(true_labels[bootstrap_indices], bootstrap_sample)\n\n    # Store the evaluation metric scores for the bootstrap sample\n    bootstrap_scores.append((bootstrap_auc, bootstrap_balanced_accuracy, bootstrap_avg_precision, bootstrap_f1_score))\n\n# Calculate the mean and standard deviation of evaluation metrics based on the bootstrap samples\nmean_auc, std_auc = np.mean([score[0] for score in bootstrap_scores]), np.std([score[0] for score in bootstrap_scores])\nmean_balanced_accuracy, std_balanced_accuracy = np.mean([score[1] for score in bootstrap_scores]), np.std([score[1] for score in bootstrap_scores])\nmean_avg_precision, std_avg_precision = np.mean([score[2] for score in bootstrap_scores]), np.std([score[2] for score in bootstrap_scores])\nmean_f1_score, std_f1_score = np.mean([score[3] for score in bootstrap_scores]), np.std([score[3] for score in bootstrap_scores])\n\n# Calculate the 95% confidence intervals\nconf_interval_auc = (mean_auc - 1.96 * std_auc, mean_auc + 1.96 * std_auc)\nconf_interval_balanced_accuracy = (mean_balanced_accuracy - 1.96 * std_balanced_accuracy, mean_balanced_accuracy + 1.96 * std_balanced_accuracy)\nconf_interval_avg_precision = (mean_avg_precision - 1.96 * std_avg_precision, mean_avg_precision + 1.96 * std_avg_precision)\nconf_interval_f1_score = (mean_f1_score - 1.96 * std_f1_score, mean_f1_score + 1.96 * std_f1_score)\n\n# Print the results\nprint(\"Original AUC:\", roc_auc_score(true_labels, MobileNetPredicted_labels))\nprint(\"95% Confidence Interval for AUC:\", conf_interval_auc)\n\nprint(\"Original Balanced Accuracy:\", balanced_accuracy_score(true_labels, MobileNetPredicted_labels))\nprint(\"95% Confidence Interval for Balanced Accuracy:\", conf_interval_balanced_accuracy)\n\nprint(\"Original Average Precision:\", average_precision_score(true_labels, MobileNetPredicted_labels))\nprint(\"95% Confidence Interval for Average Precision:\", conf_interval_avg_precision)\n\nprint(\"Original F1-score:\", f1_score(true_labels, MobileNetPredicted_labels))\nprint(\"95% Confidence Interval for F1-score:\", conf_interval_f1_score)\n","metadata":{}},{"cell_type":"markdown","source":"# Local InceptionV3 Model Classification Report","metadata":{}},{"cell_type":"code","source":"# Generate predictions for the test set\nInceptionV3Predictions = InceptionV3Model.predict_generator(test_generator)\nInceptionV3Predicted_labels = np.argmax(InceptionV3Predictions, axis=1)\nInceptionV3Report = classification_report(true_labels, InceptionV3Predicted_labels, target_names=class_names)\nprint(InceptionV3Report)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:51:24.965001Z","iopub.execute_input":"2023-10-12T10:51:24.965328Z","iopub.status.idle":"2023-10-12T10:51:31.369165Z","shell.execute_reply.started":"2023-10-12T10:51:24.965302Z","shell.execute_reply":"2023-10-12T10:51:31.368127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"import numpy as np\nfrom sklearn.metrics import roc_auc_score, balanced_accuracy_score, average_precision_score, f1_score\n\nnum_bootstrap_samples = 5000\nbootstrap_scores = []\n\nfor _ in range(num_bootstrap_samples):\n    # Generate a random bootstrap sample from the predictions with replacement\n    bootstrap_indices = np.random.choice(len(true_labels), len(true_labels), replace=True)\n    bootstrap_sample = InceptionV3Predicted_labels[bootstrap_indices]\n\n    # Calculate evaluation metrics for the bootstrap sample\n    bootstrap_auc = roc_auc_score(true_labels[bootstrap_indices], bootstrap_sample)\n    bootstrap_balanced_accuracy = balanced_accuracy_score(true_labels[bootstrap_indices], bootstrap_sample)\n    bootstrap_avg_precision = average_precision_score(true_labels[bootstrap_indices], bootstrap_sample)\n    bootstrap_f1_score = f1_score(true_labels[bootstrap_indices], bootstrap_sample)\n\n    # Store the evaluation metric scores for the bootstrap sample\n    bootstrap_scores.append((bootstrap_auc, bootstrap_balanced_accuracy, bootstrap_avg_precision, bootstrap_f1_score))\n\n# Calculate the mean and standard deviation of evaluation metrics based on the bootstrap samples\nmean_auc, std_auc = np.mean([score[0] for score in bootstrap_scores]), np.std([score[0] for score in bootstrap_scores])\nmean_balanced_accuracy, std_balanced_accuracy = np.mean([score[1] for score in bootstrap_scores]), np.std([score[1] for score in bootstrap_scores])\nmean_avg_precision, std_avg_precision = np.mean([score[2] for score in bootstrap_scores]), np.std([score[2] for score in bootstrap_scores])\nmean_f1_score, std_f1_score = np.mean([score[3] for score in bootstrap_scores]), np.std([score[3] for score in bootstrap_scores])\n\n# Calculate the 95% confidence intervals\nconf_interval_auc = (mean_auc - 1.96 * std_auc, mean_auc + 1.96 * std_auc)\nconf_interval_balanced_accuracy = (mean_balanced_accuracy - 1.96 * std_balanced_accuracy, mean_balanced_accuracy + 1.96 * std_balanced_accuracy)\nconf_interval_avg_precision = (mean_avg_precision - 1.96 * std_avg_precision, mean_avg_precision + 1.96 * std_avg_precision)\nconf_interval_f1_score = (mean_f1_score - 1.96 * std_f1_score, mean_f1_score + 1.96 * std_f1_score)\n\n# Print the results\nprint(\"Original AUC:\", roc_auc_score(true_labels, InceptionV3Predicted_labels))\nprint(\"95% Confidence Interval for AUC:\", conf_interval_auc)\n\nprint(\"Original Balanced Accuracy:\", balanced_accuracy_score(true_labels, InceptionV3Predicted_labels))\nprint(\"95% Confidence Interval for Balanced Accuracy:\", conf_interval_balanced_accuracy)\n\nprint(\"Original Average Precision:\", average_precision_score(true_labels, InceptionV3Predicted_labels))\nprint(\"95% Confidence Interval for Average Precision:\", conf_interval_avg_precision)\n\nprint(\"Original F1-score:\", f1_score(true_labels, InceptionV3Predicted_labels))\nprint(\"95% Confidence Interval for F1-score:\", conf_interval_f1_score)\n","metadata":{}},{"cell_type":"markdown","source":"# Local MobileNet Model Confusion Matrix","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nfrom mlxtend.plotting import plot_confusion_matrix\n\n\n# Load the saved model\n#model = tf.keras.models.load_model('models/final_model.h5')  # Update with the correct path\n\n# Generate predictions for the test data\n\n\n# Generate the confusion matrix\nMobileNetCM = confusion_matrix(true_labels, MobileNetPredicted_labels)\n\n# Plot confusion matrix\nplt.figure(figsize=(5, 3))\nclass_names = list(test_generator.class_indices.keys())\n#sns.heatmap(MobileNetCM, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\nplot_confusion_matrix(conf_mat=MobileNetCM, figsize=(5, 3), class_names=['Cataract', 'Normal'], show_normed=True)\nplt.title('LAAMD1 Confusion Matrix')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.tight_layout()\nplt.savefig('LAAMD1.eps',dpi=250)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:51:31.371076Z","iopub.execute_input":"2023-10-12T10:51:31.371407Z","iopub.status.idle":"2023-10-12T10:51:32.181762Z","shell.execute_reply.started":"2023-10-12T10:51:31.371374Z","shell.execute_reply":"2023-10-12T10:51:32.180852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Local Inceptionv3 Model Confusion Matrix","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Load the saved model\n#model = tf.keras.models.load_model('models/final_model.h5')  # Update with the correct path\n\n# Generate predictions for the test data\n\n\n# Generate the confusion matrix\nInceptionV3CM = confusion_matrix(true_labels, InceptionV3Predicted_labels)\n\n# Plot confusion matrix\nplt.figure(figsize=(5, 3))\nclass_names = list(test_generator.class_indices.keys())\n#sns.heatmap(InceptionV3CM, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\nplot_confusion_matrix(conf_mat=InceptionV3CM, figsize=(5, 3), class_names=['Cataract', 'Normal'], show_normed=True)\nplt.title('LAAIV3D1 Confusion Matrix')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.tight_layout()\nplt.savefig('LAAIV3D1.eps',dpi=250)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:51:36.977441Z","iopub.execute_input":"2023-10-12T10:51:36.978567Z","iopub.status.idle":"2023-10-12T10:51:37.274774Z","shell.execute_reply.started":"2023-10-12T10:51:36.978530Z","shell.execute_reply":"2023-10-12T10:51:37.273634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report, balanced_acc, avg_precision, micro_avg_precision, micro_avg_recall, micro_avg_f1_score = evaluate_classification(true_labels,  InceptionV3Predicted_labels)\n\n# Print the classification report and additional metrics\n#print(\"Classification Report:\\n\", report)\nprint(\"Micro-average F1-score:\", micro_avg_f1_score)\nprint(\"Balanced Accuracy:\", balanced_acc)\nprint(\"Average Precision:\", avg_precision)\nprint(\"Micro-average Precision:\", micro_avg_precision)\nprint(\"Micro-average Recall:\", micro_avg_recall)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:51:42.003305Z","iopub.execute_input":"2023-10-12T10:51:42.003695Z","iopub.status.idle":"2023-10-12T10:51:42.021145Z","shell.execute_reply.started":"2023-10-12T10:51:42.003625Z","shell.execute_reply":"2023-10-12T10:51:42.019774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\n\nfpr_model1, tpr_model1, thresholds_model1 = roc_curve(true_labels, MobileNetPredicted_labels)\nauc_model1 = roc_auc_score(true_labels, MobileNetPredicted_labels)\n\n# Calculate ROC curve and AUC for Model 2\nfpr_model2, tpr_model2, thresholds_model2 = roc_curve(true_labels, InceptionV3Predicted_labels)\nauc_model2 = roc_auc_score(true_labels, InceptionV3Predicted_labels)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:51:45.121261Z","iopub.execute_input":"2023-10-12T10:51:45.122357Z","iopub.status.idle":"2023-10-12T10:51:45.135004Z","shell.execute_reply.started":"2023-10-12T10:51:45.122318Z","shell.execute_reply":"2023-10-12T10:51:45.134080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot ROC curves\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_model1, tpr_model1, label='LocalMobileNetD1 (AUC = {:.2f})'.format(auc_model1))\nplt.plot(fpr_model2, tpr_model2, label='LocalInceptionV3D1 (AUC = {:.2f})'.format(auc_model2))\nplt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve Comparison')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:51:48.298407Z","iopub.execute_input":"2023-10-12T10:51:48.299353Z","iopub.status.idle":"2023-10-12T10:51:48.688204Z","shell.execute_reply.started":"2023-10-12T10:51:48.299314Z","shell.execute_reply":"2023-10-12T10:51:48.687027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n\n# Assuming you have predictions and true labels for both models\n# Replace 'MobileNetPredicted_labels' and 'InceptionV3Predicted_labels' with the predictions of your models\n# Replace 'true_labels' with the true labels of your data\n\n# Calculate precision-recall curve for Model 1\nprecision_model1, recall_model1, thresholds_model1 = precision_recall_curve(true_labels, MobileNetPredicted_labels)\n\n# Calculate precision-recall curve for Model 2\nprecision_model2, recall_model2, thresholds_model2 = precision_recall_curve(true_labels, InceptionV3Predicted_labels)\n\nimport matplotlib.pyplot as plt\n\n# Plot the precision-recall curves for both models\nplt.figure(figsize=(8, 6))\nplt.plot(recall_model1, precision_model1, marker='.', label='LocalMobileNetD1')\nplt.plot(recall_model2, precision_model2, marker='.', label='LocalInceptionV3D1')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve Comparison')\nplt.legend()\n\n# Annotate points with precision-recall values for Model 1\nfor p, r, t in zip(precision_model1, recall_model1, thresholds_model1):\n    plt.annotate(f'LGMobileNet: {t:.2f}\\n{p:.2f}/{r:.2f}', xy=(r, p), xytext=(r + 0.03, p), arrowprops=dict(arrowstyle='->'), fontsize=8)\n\n# Annotate points with precision-recall values for Model 2\nfor p, r, t in zip(precision_model2, recall_model2, thresholds_model2):\n    plt.annotate(f'LGInceptionV3: {t:.2f}\\n{p:.2f}/{r:.2f}', xy=(r, p), xytext=(r - 0.15, p), arrowprops=dict(arrowstyle='->'), fontsize=8)\n\nplt.grid(True)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:51:53.442513Z","iopub.execute_input":"2023-10-12T10:51:53.443149Z","iopub.status.idle":"2023-10-12T10:51:53.797678Z","shell.execute_reply.started":"2023-10-12T10:51:53.443117Z","shell.execute_reply":"2023-10-12T10:51:53.796848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MobileNetModel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport random\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\n# Step 1: Load and preprocess the image data (Replace 'your_image_path' with the image you want to visualize)\n# Step 1: Load and preprocess the image data\ntest_folder = '/kaggle/input/cataract-image-dataset/processed_images/test'\ninput_size = (224, 224)  # Input size of the model, adjust according to your model's input size\n# Step 2: Pick 4 sample images (2 from 'cataract' class and 2 from 'normal' class)\nclass_folders = ['cataract', 'normal']\nsample_images = []\n\nfor class_folder in class_folders:\n    class_path = os.path.join(test_folder, class_folder)\n    images = os.listdir(class_path)\n    sample_images.extend(random.sample(images, 2))\n\ndef preprocess_image(image_path, input_size):\n    img = load_img(image_path, target_size=input_size)\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = tf.keras.applications.vgg16.preprocess_input(img)  # Preprocess based on VGG16 requirements\n    return img\n\n\n# Step 3: Define a function to compute the Grad-CAM visualization\ndef compute_gradcam(model, image):\n    # Get the last convolutional layer and the output layer of the model\n    last_conv_layer = model.get_layer('multiply')\n    output_layer = model.layers[-1]\n\n    # Create a model that maps the input image to the output class predictions and the last convolutional layer\n    grad_model = Model(inputs=model.input, outputs=[last_conv_layer.output, output_layer.output])\n\n    # Compute the gradients of the predicted class with respect to the last convolutional layer\n    with tf.GradientTape() as tape:\n        conv_output, predictions = grad_model(image)\n        loss = predictions[:, 0]  # Assuming binary classification, change this if you have different output classes\n\n    grads = tape.gradient(loss, conv_output)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # Multiply each channel in the feature map by its corresponding gradient importance\n    conv_output = conv_output[0]\n    heatmap = tf.reduce_sum(tf.multiply(conv_output, pooled_grads), axis=-1)\n    heatmap = np.maximum(heatmap, 0)  # Apply ReLU activation\n    heatmap /= np.max(heatmap)  # Normalize the heatmap values between 0 and 1\n\n    return heatmap\n\n\n# Create a figure with subplots for original images and their Grad-CAM images\nnum_images = len(sample_images)\nfig, axes = plt.subplots(2, num_images, figsize=(4*num_images, 8))\n\n# Generate Grad-CAM for each image and plot the results\nfor i, image_name in enumerate(sample_images):\n    class_folder = 'cataract' if 'cataract' in image_name else 'normal'\n    image_path = os.path.join(test_folder, class_folder, image_name)\n    image = preprocess_image(image_path, input_size)\n    heatmap = compute_gradcam(MobileNetModel, image)\n    heatmap = cv2.resize(heatmap, (input_size[1], input_size[0]))\n    heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n\n    # Load the original image\n    original_image = cv2.imread(image_path)\n    original_image = cv2.resize(original_image, (input_size[1], input_size[0]))\n\n    # Overlay the heatmap on the original image\n    alpha = 0.5  # Adjust the alpha value for the heatmap overlay\n    superimposed_img = cv2.addWeighted(original_image, alpha, heatmap, 1 - alpha, 0)\n\n    # Display the original image and Grad-CAM side by side\n    axes[0, i].imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n    axes[0, i].set_title('Original Image')\n    axes[0, i].axis('off')\n\n    axes[1, i].imshow(cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB))\n    axes[1, i].set_title('Grad-CAM Visualization')\n    axes[1, i].axis('off')\n\nplt.tight_layout()\n#plt.savefig('LGAAM_D1_GradCamVisualization.eps',dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:52:30.950756Z","iopub.execute_input":"2023-10-12T10:52:30.951105Z","iopub.status.idle":"2023-10-12T10:52:32.895033Z","shell.execute_reply.started":"2023-10-12T10:52:30.951077Z","shell.execute_reply":"2023-10-12T10:52:32.893922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the Grad-CAM heatmap:\n\nRegions with warm colors (e.g., red, orange, and yellow) indicate high importance or high activation. \nThese regions are crucial for the model's prediction, and the model relies heavily on the features extracted \nfrom these areas to make its decision.\n\nRegions with cool colors (e.g., blue and green) indicate low importance or low activation. \nThese regions are less relevant to the model's prediction, and the model does not rely much on \nthe features from these areas to make its decision.\n\nIn a binary classification scenario like this, where the model predicts between 'cataract' and 'normal' \nclasses, the heatmap will show which parts of the image the model considers important for classifying \nthe input as 'cataract' (warm regions) and which parts are not as relevant for the 'normal' class (cool regions).","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.applications import MobileNet, InceptionV3\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Path to the dataset\ndataset_dir = 'd:/chaman/cataract_dataset3'\ntrain_dir = os.path.join(dataset_dir, 'train')\ntest_dir = os.path.join(dataset_dir, 'test')\n\n# Image size for the models\nimage_size = (224, 224)\nbatch_size = 32\nnum_classes = 2\nepochs = 10\n\n# Implement the data loading and preparation functions\ndef prepare_data():\n    datagen = ImageDataGenerator(rescale=1.0/255.0,rotation_range=20,width_shift_range=0.2,height_shift_range=0.2,\n    shear_range=0.2,zoom_range=0.2,horizontal_flip=True, validation_split=0.2)\n    train_generator = datagen.flow_from_directory(train_dir, target_size=image_size, batch_size=batch_size,\n                                                  class_mode='categorical', subset='training', shuffle=True)\n    val_generator = datagen.flow_from_directory(train_dir, target_size=image_size, batch_size=batch_size,\n                                                class_mode='categorical', subset='validation', shuffle=False)\n\n    return train_generator, val_generator\n\ndef train_model(model_fn, input_shape, num_classes, train_generator, val_generator):\n    model = model_fn(input_shape, num_classes)\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    model.fit(train_generator, epochs=epochs, validation_data=val_generator)\n    return model\n\ndef bagging_ensemble(models, val_generator):\n    # Make predictions for each model\n    predictions = [model.predict(val_generator) for model in models]\n    # Average the predictions of all models\n    return np.mean(predictions, axis=0)\n\ndef stacking_ensemble(models, meta_model, val_generator):\n    # Make predictions for each model\n    predictions = [model.predict(val_generator) for model in models]\n    stacked_predictions = np.hstack(predictions)\n\n    # Use the stacked predictions to train the meta-model\n    meta_model.fit(stacked_predictions, val_generator.classes, epochs=epochs)\n\n    # Make predictions using the meta-model\n    ensemble_predictions = [model.predict(val_generator) for model in models]\n    stacked_ensemble_predictions = np.hstack(ensemble_predictions)\n    meta_predictions = meta_model.predict(stacked_ensemble_predictions)\n\n    return meta_predictions\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Prepare the data\n    train_generator, val_generator = prepare_data()\n\n    # Ensemble size (the number of models in the ensemble)\n    ensemble_size = 5\n\n    # Create and train ensemble models\n    ensemble_models = []\n    for i in range(ensemble_size):\n        # Bootstrap sampling (with replacement) for training data\n        train_generator_sample = train_generator\n        model_fn = create_Local_attention_augmented_mobilenet if i % 2 == 0 else create_Local_Inceptionv3_model\n        model = train_model(model_fn, image_size + (3,), num_classes, train_generator_sample, val_generator)\n        ensemble_models.append(model)\n\n    # Create the meta-model for stacking\n    input_shape = (ensemble_size * num_classes,)\n    meta_model = tf.keras.Sequential([\n        Input(shape=input_shape),\n        Dense(64, activation='relu'),\n        Dense(num_classes, activation='softmax')\n    ])\n    meta_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n    # Make predictions using the stacking ensemble method\n    stacking_predictions = stacking_ensemble(ensemble_models, meta_model, val_generator)\n\n    # Threshold predictions for binary classification\n    y_val = val_generator.classes\n    stacking_predictions_binary = (stacking_predictions.argmax(axis=1) > 0.5).astype(int)\n\n    # Evaluate the stacking ensemble performance\n    stacking_accuracy = np.mean(stacking_predictions_binary == y_val)\n    print(f\"Stacking Ensemble Accuracy: {stacking_accuracy}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the test data using the stacking ensemble method\ntest_stacking_predictions = stacking_ensemble(ensemble_models, meta_model, test_generator)\n\n    # Threshold predictions for binary classification (if necessary)\ntest_stacking_predictions_binary = (test_stacking_predictions.argmax(axis=1) > 0.5).astype(int)\n\n    # Evaluate the stacking ensemble performance on test data\ny_test = test_generator.classes\ntest_stacking_accuracy = np.mean(test_stacking_predictions_binary == y_test)\nprint(f\"Stacking Ensemble Accuracy on Test Data: {test_stacking_accuracy}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate the classification report\nclass_names = test_generator.class_indices\nclass_names = [class_name for class_name, index in sorted(class_names.items(), key=lambda x: x[1])]\nreport = classification_report(y_test, test_stacking_predictions_binary, target_names=class_names)\nprint(\"Classification Report:\")\nprint(report)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport bootstrapped.bootstrap as bs\nimport bootstrapped.stats_functions as bs_stats\nfrom sklearn.metrics import roc_auc_score, balanced_accuracy_score, f1_score\n\n# Assuming you have your true_labels and MobileNetPredicted_labels defined\n\n# Function to calculate AUC for a given sample\ndef calculate_auc(sample):\n    return roc_auc_score(true_labels[sample], MobileNetPredicted_labels[sample])\n\n# Function to calculate balanced accuracy for a given sample (micro-averaged)\ndef calculate_balanced_accuracy(sample):\n    return balanced_accuracy_score(true_labels[sample], MobileNetPredicted_labels[sample], adjusted=True)\n\n# Function to calculate F1-score for a given sample (micro-averaged)\ndef calculate_f1_score(sample):\n    return f1_score(true_labels[sample], MobileNetPredicted_labels[sample], average='micro')\n\n# Number of bootstrap samples\nnum_bootstrap_samples = 5000\n\n# Generate bootstrap indices (bootstrap_samples) for each iteration\nbootstrap_samples = [np.random.choice(len(true_labels), len(true_labels), replace=True) for _ in range(num_bootstrap_samples)]\n\n# Calculate bootstrap AUCs\nbootstrap_aucs = np.array([calculate_auc(sample) for sample in bootstrap_samples])\n\n# Calculate bootstrap balanced accuracies (micro-averaged)\nbootstrap_balanced_accuracies = np.array([calculate_balanced_accuracy(sample) for sample in bootstrap_samples])\n\n# Calculate bootstrap F1-scores (micro-averaged)\nbootstrap_f1_scores = np.array([calculate_f1_score(sample) for sample in bootstrap_samples])\n\n# Calculate bootstrap confidence intervals using bootstrapped library\nauc_ci = bs.bootstrap(bootstrap_aucs, stat_func=bs_stats.mean, alpha=0.05)\nbalanced_accuracy_ci = bs.bootstrap(bootstrap_balanced_accuracies, stat_func=bs_stats.mean, alpha=0.05)\nf1_score_ci = bs.bootstrap(bootstrap_f1_scores, stat_func=bs_stats.mean, alpha=0.05)\n\n# Print the results\nprint(\"Original AUC:\", roc_auc_score(true_labels, MobileNetPredicted_labels))\nprint(\"95% Confidence Interval for AUC:\", auc_ci)\n\nprint(\"Original Balanced Accuracy (Micro-averaged):\", balanced_accuracy_score(true_labels, MobileNetPredicted_labels, adjusted=True))\nprint(\"95% Confidence Interval for Balanced Accuracy (Micro-averaged):\", balanced_accuracy_ci)\n\nprint(\"Original F1-score (Micro-averaged):\", f1_score(true_labels, MobileNetPredicted_labels, average='micro'))\nprint(\"95% Confidence Interval for F1-score (Micro-averaged):\", f1_score_ci)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, multiply, Permute, Concatenate, Conv2D, Add, Activation, Lambda\nfrom keras import backend as K\nfrom keras.activations import sigmoid\nMobileNetModelWA = create_Local_attention_augmented_mobilenet(input_shape + (3,), num_classes,attention=False)\n\n# 4. Compile the model\nMobileNetModelWA.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T13:15:27.447385Z","iopub.execute_input":"2023-10-12T13:15:27.447727Z","iopub.status.idle":"2023-10-12T13:15:28.336640Z","shell.execute_reply.started":"2023-10-12T13:15:27.447701Z","shell.execute_reply":"2023-10-12T13:15:28.335700Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"InceptionV3ModelWA = create_Local_Inceptionv3_model(input_shape + (3,), num_classes,attention=False)\n\nInceptionV3ModelWA.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T13:15:43.845830Z","iopub.execute_input":"2023-10-12T13:15:43.846159Z","iopub.status.idle":"2023-10-12T13:15:46.670614Z","shell.execute_reply.started":"2023-10-12T13:15:43.846130Z","shell.execute_reply":"2023-10-12T13:15:46.669620Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"epochs=100\n# Create a directory to save the best model weights during training\nos.makedirs('models', exist_ok=True)\nmodel_checkpoint = ModelCheckpoint('models/best_LocalInceptionModel1.h5', save_best_only=True, save_weights_only=True)\n\nInceptionV3historyWA = InceptionV3ModelWA.fit(\n    train_generator,\n    steps_per_epoch=train_generator.n // train_generator.batch_size,\n    epochs=epochs,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.n // validation_generator.batch_size,\n    callbacks=[model_checkpoint,cp_callback2]\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T13:17:11.421262Z","iopub.execute_input":"2023-10-12T13:17:11.421630Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/100\n12/12 [==============================] - ETA: 0s - loss: 1.9280 - accuracy: 0.6731\nEpoch 1: val_loss improved from inf to 0.35506, saving model to ./fpseweights1.h5\n12/12 [==============================] - 38s 2s/step - loss: 1.9280 - accuracy: 0.6731 - val_loss: 0.3551 - val_accuracy: 0.8854\nEpoch 2/100\n12/12 [==============================] - ETA: 0s - loss: 0.4964 - accuracy: 0.8393\nEpoch 2: val_loss improved from 0.35506 to 0.26344, saving model to ./fpseweights1.h5\n12/12 [==============================] - 19s 2s/step - loss: 0.4964 - accuracy: 0.8393 - val_loss: 0.2634 - val_accuracy: 0.9062\nEpoch 3/100\n12/12 [==============================] - ETA: 0s - loss: 0.2706 - accuracy: 0.8864\nEpoch 3: val_loss improved from 0.26344 to 0.25609, saving model to ./fpseweights1.h5\n12/12 [==============================] - 21s 2s/step - loss: 0.2706 - accuracy: 0.8864 - val_loss: 0.2561 - val_accuracy: 0.8958\nEpoch 4/100\n12/12 [==============================] - ETA: 0s - loss: 0.2919 - accuracy: 0.8726\nEpoch 4: val_loss improved from 0.25609 to 0.20626, saving model to ./fpseweights1.h5\n12/12 [==============================] - 18s 2s/step - loss: 0.2919 - accuracy: 0.8726 - val_loss: 0.2063 - val_accuracy: 0.8854\nEpoch 5/100\n12/12 [==============================] - ETA: 0s - loss: 0.2605 - accuracy: 0.8802\nEpoch 5: val_loss improved from 0.20626 to 0.18440, saving model to ./fpseweights1.h5\n12/12 [==============================] - 20s 2s/step - loss: 0.2605 - accuracy: 0.8802 - val_loss: 0.1844 - val_accuracy: 0.9167\nEpoch 6/100\n12/12 [==============================] - ETA: 0s - loss: 0.2065 - accuracy: 0.9224\nEpoch 6: val_loss improved from 0.18440 to 0.15838, saving model to ./fpseweights1.h5\n12/12 [==============================] - 21s 2s/step - loss: 0.2065 - accuracy: 0.9224 - val_loss: 0.1584 - val_accuracy: 0.9167\nEpoch 7/100\n12/12 [==============================] - ETA: 0s - loss: 0.1811 - accuracy: 0.9418\nEpoch 7: val_loss improved from 0.15838 to 0.12626, saving model to ./fpseweights1.h5\n12/12 [==============================] - 19s 2s/step - loss: 0.1811 - accuracy: 0.9418 - val_loss: 0.1263 - val_accuracy: 0.9375\nEpoch 8/100\n12/12 [==============================] - ETA: 0s - loss: 0.1816 - accuracy: 0.9307\nEpoch 8: val_loss did not improve from 0.12626\n12/12 [==============================] - 20s 2s/step - loss: 0.1816 - accuracy: 0.9307 - val_loss: 0.1595 - val_accuracy: 0.9167\nEpoch 9/100\n12/12 [==============================] - ETA: 0s - loss: 0.1985 - accuracy: 0.9252\nEpoch 9: val_loss improved from 0.12626 to 0.08344, saving model to ./fpseweights1.h5\n12/12 [==============================] - 21s 2s/step - loss: 0.1985 - accuracy: 0.9252 - val_loss: 0.0834 - val_accuracy: 0.9792\nEpoch 10/100\n12/12 [==============================] - ETA: 0s - loss: 0.1698 - accuracy: 0.9335\nEpoch 10: val_loss did not improve from 0.08344\n12/12 [==============================] - 20s 2s/step - loss: 0.1698 - accuracy: 0.9335 - val_loss: 0.1502 - val_accuracy: 0.9167\nEpoch 11/100\n12/12 [==============================] - ETA: 0s - loss: 0.1897 - accuracy: 0.9252\nEpoch 11: val_loss did not improve from 0.08344\n12/12 [==============================] - 20s 2s/step - loss: 0.1897 - accuracy: 0.9252 - val_loss: 0.1125 - val_accuracy: 0.9479\nEpoch 12/100\n12/12 [==============================] - ETA: 0s - loss: 0.1597 - accuracy: 0.9446\nEpoch 12: val_loss did not improve from 0.08344\n12/12 [==============================] - 20s 2s/step - loss: 0.1597 - accuracy: 0.9446 - val_loss: 0.0886 - val_accuracy: 0.9688\nEpoch 13/100\n12/12 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9474\nEpoch 13: val_loss did not improve from 0.08344\n12/12 [==============================] - 18s 2s/step - loss: 0.1406 - accuracy: 0.9474 - val_loss: 0.1399 - val_accuracy: 0.9375\nEpoch 14/100\n12/12 [==============================] - ETA: 0s - loss: 0.1656 - accuracy: 0.9335\nEpoch 14: val_loss did not improve from 0.08344\n12/12 [==============================] - 18s 2s/step - loss: 0.1656 - accuracy: 0.9335 - val_loss: 0.1863 - val_accuracy: 0.9271\nEpoch 15/100\n12/12 [==============================] - ETA: 0s - loss: 0.1484 - accuracy: 0.9446\nEpoch 15: val_loss did not improve from 0.08344\n12/12 [==============================] - 19s 2s/step - loss: 0.1484 - accuracy: 0.9446 - val_loss: 0.1231 - val_accuracy: 0.9479\nEpoch 16/100\n12/12 [==============================] - ETA: 0s - loss: 0.1518 - accuracy: 0.9501\nEpoch 16: val_loss did not improve from 0.08344\n12/12 [==============================] - 17s 1s/step - loss: 0.1518 - accuracy: 0.9501 - val_loss: 0.1550 - val_accuracy: 0.9375\nEpoch 17/100\n12/12 [==============================] - ETA: 0s - loss: 0.1373 - accuracy: 0.9584\nEpoch 17: val_loss did not improve from 0.08344\n12/12 [==============================] - 18s 2s/step - loss: 0.1373 - accuracy: 0.9584 - val_loss: 0.1143 - val_accuracy: 0.9583\nEpoch 18/100\n12/12 [==============================] - ETA: 0s - loss: 0.1276 - accuracy: 0.9557\nEpoch 18: val_loss did not improve from 0.08344\n12/12 [==============================] - 18s 2s/step - loss: 0.1276 - accuracy: 0.9557 - val_loss: 0.1215 - val_accuracy: 0.9479\nEpoch 19/100\n12/12 [==============================] - ETA: 0s - loss: 0.1324 - accuracy: 0.9418\nEpoch 19: val_loss did not improve from 0.08344\n12/12 [==============================] - 20s 2s/step - loss: 0.1324 - accuracy: 0.9418 - val_loss: 0.1149 - val_accuracy: 0.9375\nEpoch 20/100\n12/12 [==============================] - ETA: 0s - loss: 0.1447 - accuracy: 0.9529\nEpoch 20: val_loss did not improve from 0.08344\n12/12 [==============================] - 18s 2s/step - loss: 0.1447 - accuracy: 0.9529 - val_loss: 0.1358 - val_accuracy: 0.9271\nEpoch 21/100\n12/12 [==============================] - ETA: 0s - loss: 0.1660 - accuracy: 0.9453\nEpoch 21: val_loss did not improve from 0.08344\n12/12 [==============================] - 18s 2s/step - loss: 0.1660 - accuracy: 0.9453 - val_loss: 0.1107 - val_accuracy: 0.9479\nEpoch 22/100\n12/12 [==============================] - ETA: 0s - loss: 0.1230 - accuracy: 0.9391\nEpoch 22: val_loss did not improve from 0.08344\n12/12 [==============================] - 20s 2s/step - loss: 0.1230 - accuracy: 0.9391 - val_loss: 0.1360 - val_accuracy: 0.9271\nEpoch 23/100\n12/12 [==============================] - ETA: 0s - loss: 0.1452 - accuracy: 0.9474\nEpoch 23: val_loss did not improve from 0.08344\n12/12 [==============================] - 20s 2s/step - loss: 0.1452 - accuracy: 0.9474 - val_loss: 0.1695 - val_accuracy: 0.9062\nEpoch 24/100\n12/12 [==============================] - ETA: 0s - loss: 0.1218 - accuracy: 0.9501\nEpoch 24: val_loss did not improve from 0.08344\n12/12 [==============================] - 20s 2s/step - loss: 0.1218 - accuracy: 0.9501 - val_loss: 0.1012 - val_accuracy: 0.9479\nEpoch 25/100\n12/12 [==============================] - ETA: 0s - loss: 0.1341 - accuracy: 0.9584\nEpoch 25: val_loss did not improve from 0.08344\n12/12 [==============================] - 21s 2s/step - loss: 0.1341 - accuracy: 0.9584 - val_loss: 0.0919 - val_accuracy: 0.9479\nEpoch 26/100\n12/12 [==============================] - ETA: 0s - loss: 0.1150 - accuracy: 0.9557\nEpoch 26: val_loss did not improve from 0.08344\n12/12 [==============================] - 20s 2s/step - loss: 0.1150 - accuracy: 0.9557 - val_loss: 0.1791 - val_accuracy: 0.9062\nEpoch 27/100\n12/12 [==============================] - ETA: 0s - loss: 0.1659 - accuracy: 0.9335\nEpoch 27: val_loss did not improve from 0.08344\n12/12 [==============================] - 19s 2s/step - loss: 0.1659 - accuracy: 0.9335 - val_loss: 0.1502 - val_accuracy: 0.9583\nEpoch 28/100\n12/12 [==============================] - ETA: 0s - loss: 0.1726 - accuracy: 0.9418\nEpoch 28: val_loss did not improve from 0.08344\n12/12 [==============================] - 19s 2s/step - loss: 0.1726 - accuracy: 0.9418 - val_loss: 0.1015 - val_accuracy: 0.9479\nEpoch 29/100\n12/12 [==============================] - ETA: 0s - loss: 0.1705 - accuracy: 0.9391\nEpoch 29: val_loss did not improve from 0.08344\n12/12 [==============================] - 19s 2s/step - loss: 0.1705 - accuracy: 0.9391 - val_loss: 0.2443 - val_accuracy: 0.9375\nEpoch 30/100\n12/12 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9668\nEpoch 30: val_loss did not improve from 0.08344\n12/12 [==============================] - 18s 2s/step - loss: 0.1057 - accuracy: 0.9668 - val_loss: 0.1062 - val_accuracy: 0.9375\nEpoch 31/100\n12/12 [==============================] - ETA: 0s - loss: 0.0809 - accuracy: 0.9778\nEpoch 31: val_loss did not improve from 0.08344\n12/12 [==============================] - 18s 1s/step - loss: 0.0809 - accuracy: 0.9778 - val_loss: 0.1166 - val_accuracy: 0.9479\nEpoch 32/100\n12/12 [==============================] - ETA: 0s - loss: 0.1857 - accuracy: 0.9197\nEpoch 32: val_loss did not improve from 0.08344\n12/12 [==============================] - 20s 2s/step - loss: 0.1857 - accuracy: 0.9197 - val_loss: 0.1192 - val_accuracy: 0.9688\nEpoch 33/100\n12/12 [==============================] - ETA: 0s - loss: 0.1059 - accuracy: 0.9557\nEpoch 33: val_loss did not improve from 0.08344\n12/12 [==============================] - 19s 2s/step - loss: 0.1059 - accuracy: 0.9557 - val_loss: 0.0908 - val_accuracy: 0.9583\nEpoch 34/100\n12/12 [==============================] - ETA: 0s - loss: 0.1157 - accuracy: 0.9474\nEpoch 34: val_loss did not improve from 0.08344\n12/12 [==============================] - 17s 1s/step - loss: 0.1157 - accuracy: 0.9474 - val_loss: 0.0934 - val_accuracy: 0.9479\nEpoch 35/100\n12/12 [==============================] - ETA: 0s - loss: 0.0866 - accuracy: 0.9640\nEpoch 35: val_loss did not improve from 0.08344\n12/12 [==============================] - 18s 1s/step - loss: 0.0866 - accuracy: 0.9640 - val_loss: 0.1191 - val_accuracy: 0.9479\nEpoch 36/100\n12/12 [==============================] - ETA: 0s - loss: 0.0938 - accuracy: 0.9740\nEpoch 36: val_loss improved from 0.08344 to 0.06796, saving model to ./fpseweights1.h5\n12/12 [==============================] - 22s 2s/step - loss: 0.0938 - accuracy: 0.9740 - val_loss: 0.0680 - val_accuracy: 0.9792\nEpoch 37/100\n12/12 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9723\nEpoch 37: val_loss improved from 0.06796 to 0.05565, saving model to ./fpseweights1.h5\n12/12 [==============================] - 21s 2s/step - loss: 0.0804 - accuracy: 0.9723 - val_loss: 0.0556 - val_accuracy: 0.9688\nEpoch 38/100\n12/12 [==============================] - ETA: 0s - loss: 0.0999 - accuracy: 0.9612\nEpoch 38: val_loss did not improve from 0.05565\n12/12 [==============================] - 19s 2s/step - loss: 0.0999 - accuracy: 0.9612 - val_loss: 0.1947 - val_accuracy: 0.9271\nEpoch 39/100\n 7/12 [================>.............] - ETA: 7s - loss: 0.1197 - accuracy: 0.9502","output_type":"stream"}]},{"cell_type":"code","source":"# 6. Evaluate the model\nInceptionV3WAEvaluation = InceptionV3ModelWA.evaluate(validation_generator)\nprint(\"Validation Accuracy: {:.2f}%\".format(InceptionV3WAEvaluation[1] * 100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on the test set\nMobileNetTestEvaluationWA = MobileNetModelWA.evaluate_generator(test_generator)\nprint(\"Test Accuracy: {:.2f}%\".format(MobileNetTestEvaluationWA[1] * 100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}