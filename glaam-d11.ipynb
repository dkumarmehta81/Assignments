{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.backend import function\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.keras.layers import Input, Multiply\nfrom tensorflow.keras.applications import InceptionV3,MobileNet","metadata":{"execution":{"iopub.status.busy":"2023-10-11T12:52:06.193800Z","iopub.execute_input":"2023-10-11T12:52:06.194262Z","iopub.status.idle":"2023-10-11T12:52:06.201130Z","shell.execute_reply.started":"2023-10-11T12:52:06.194225Z","shell.execute_reply":"2023-10-11T12:52:06.200232Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Set a random seed for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T12:52:11.125627Z","iopub.execute_input":"2023-10-11T12:52:11.125982Z","iopub.status.idle":"2023-10-11T12:52:11.130476Z","shell.execute_reply.started":"2023-10-11T12:52:11.125954Z","shell.execute_reply":"2023-10-11T12:52:11.129495Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Global Attention Module Declaration","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Dense, Reshape, Multiply, Add, Conv2D, BatchNormalization, ReLU, Input, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\ndef channel_attention(input, ratio=16):\n    avg_pool = GlobalAveragePooling2D()(input)\n    max_pool = GlobalMaxPooling2D()(input)\n\n    channel_avg = Dense(units=input.shape[-1] // ratio, activation='relu',kernel_initializer='he_normal',use_bias=True,bias_initializer='zeros')(avg_pool)\n    channel_max = Dense(units=input.shape[-1] // ratio, activation='relu',kernel_initializer='he_normal',use_bias=True,bias_initializer='zeros')(max_pool)\n\n    channel_avg = Dense(units=input.shape[-1], activation='sigmoid',kernel_initializer='he_normal',use_bias=True)(channel_avg)\n    channel_max = Dense(units=input.shape[-1], activation='sigmoid',kernel_initializer='he_normal',use_bias=True)(channel_max)\n\n    channel_attention = Add()([channel_avg, channel_max])\n    channel_attention = Multiply()([input, Reshape((1, 1, input.shape[-1]))(channel_attention)])\n\n    return channel_attention","metadata":{"execution":{"iopub.status.busy":"2023-10-11T12:52:14.927479Z","iopub.execute_input":"2023-10-11T12:52:14.927843Z","iopub.status.idle":"2023-10-11T12:52:14.936783Z","shell.execute_reply.started":"2023-10-11T12:52:14.927814Z","shell.execute_reply":"2023-10-11T12:52:14.935831Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, multiply, Permute, Concatenate, Conv2D, Add, Activation, Lambda\nfrom keras import backend as K\nfrom keras.activations import sigmoid\n\ndef attach_attention_module(net, attention_module):\n  if attention_module == 'se_block': # SE_block\n    net = se_block(net)\n  elif attention_module == 'cbam_block': # CBAM_block\n    net = cbam_block(net)\n  else:\n    raise Exception(\"'{}' is not supported attention module!\".format(attention_module))\n\n  return net\n\ndef se_block(input_feature, ratio=8):\n\t\"\"\"Contains the implementation of Squeeze-and-Excitation(SE) block.\n\tAs described in https://arxiv.org/abs/1709.01507.\n\t\"\"\"\n\t\n\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n\tchannel = input_feature.shape[channel_axis]\n\n\tse_feature = GlobalAveragePooling2D()(input_feature)\n\tse_feature = Reshape((1, 1, channel))(se_feature)\n\tassert se_feature.shape[1:] == (1,1,channel)\n\tse_feature = Dense(channel // ratio,\n\t\t\t\t\t   activation='relu',\n\t\t\t\t\t   kernel_initializer='he_normal',\n\t\t\t\t\t   use_bias=True,\n\t\t\t\t\t   bias_initializer='zeros')(se_feature)\n\tassert se_feature.shape[1:] == (1,1,channel//ratio)\n\tse_feature = Dense(channel,\n\t\t\t\t\t   activation='sigmoid',\n\t\t\t\t\t   kernel_initializer='he_normal',\n\t\t\t\t\t   use_bias=True,\n\t\t\t\t\t   bias_initializer='zeros')(se_feature)\n\tassert se_feature.shape[1:] == (1,1,channel)\n\tif K.image_data_format() == 'channels_first':\n\t\tse_feature = Permute((3, 1, 2))(se_feature)\n\n\tse_feature = multiply([input_feature, se_feature])\n\treturn se_feature\n\ndef cbam_block(cbam_feature, ratio=8):\n\t\"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n\tAs described in https://arxiv.org/abs/1807.06521.\n\t\"\"\"\n\t\n\tcbam_feature = channel_attention(cbam_feature, ratio)\n\tcbam_feature = spatial_attention(cbam_feature)\n\treturn cbam_feature\n\ndef channel_attention(input_feature, ratio=8):\n\t\n\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n\tchannel = input_feature.shape[channel_axis]\n\t\n\tshared_layer_one = Dense(channel//ratio,\n\t\t\t\t\t\t\t activation='relu',\n\t\t\t\t\t\t\t kernel_initializer='he_normal',\n\t\t\t\t\t\t\t use_bias=True,\n\t\t\t\t\t\t\t bias_initializer='zeros')\n\tshared_layer_two = Dense(channel,\n\t\t\t\t\t\t\t kernel_initializer='he_normal',\n\t\t\t\t\t\t\t use_bias=True,\n\t\t\t\t\t\t\t bias_initializer='zeros')\n\t\n\tavg_pool = GlobalAveragePooling2D()(input_feature)    \n\tavg_pool = Reshape((1,1,channel))(avg_pool)\n\tassert avg_pool.shape[1:] == (1,1,channel)\n\tavg_pool = shared_layer_one(avg_pool)\n\tassert avg_pool.shape[1:] == (1,1,channel//ratio)\n\tavg_pool = shared_layer_two(avg_pool)\n\tassert avg_pool.shape[1:] == (1,1,channel)\n\t\n\tmax_pool = GlobalMaxPooling2D()(input_feature)\n\tmax_pool = Reshape((1,1,channel))(max_pool)\n\tassert max_pool.shape[1:] == (1,1,channel)\n\tmax_pool = shared_layer_one(max_pool)\n\tassert max_pool.shape[1:] == (1,1,channel//ratio)\n\tmax_pool = shared_layer_two(max_pool)\n\tassert max_pool.shape[1:] == (1,1,channel)\n\t\n\tcbam_feature = Add()([avg_pool,max_pool])\n\tcbam_feature = Activation('sigmoid')(cbam_feature)\n\t\n\tif K.image_data_format() == \"channels_first\":\n\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n\t\n\treturn multiply([input_feature, cbam_feature])\n\ndef spatial_attention(input_feature):\n\tkernel_size = 7\n\t\n\tif K.image_data_format() == \"channels_first\":\n\t\tchannel = input_feature.shape[1]\n\t\tcbam_feature = Permute((2,3,1))(input_feature)\n\telse:\n\t\tchannel = input_feature.shape[-1]\n\t\tcbam_feature = input_feature\n\t\n\tavg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n\tassert avg_pool.shape[-1] == 1\n\tmax_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n\tassert max_pool.shape[-1] == 1\n\tconcat = Concatenate(axis=3)([avg_pool, max_pool])\n\tassert concat.shape[-1] == 2\n\tcbam_feature = Conv2D(filters = 1,\n\t\t\t\t\tkernel_size=kernel_size,\n\t\t\t\t\tstrides=1,\n\t\t\t\t\tpadding='same',\n\t\t\t\t\tactivation='sigmoid',\n\t\t\t\t\tkernel_initializer='he_normal',\n\t\t\t\t\tuse_bias=False)(concat)\t\n\tassert cbam_feature.shape[-1] == 1\n\t\n\tif K.image_data_format() == \"channels_first\":\n\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n\t\t\n\treturn multiply([input_feature, cbam_feature])\n\t\t","metadata":{"execution":{"iopub.status.busy":"2023-10-11T12:52:20.837812Z","iopub.execute_input":"2023-10-11T12:52:20.838192Z","iopub.status.idle":"2023-10-11T12:52:20.854758Z","shell.execute_reply.started":"2023-10-11T12:52:20.838163Z","shell.execute_reply":"2023-10-11T12:52:20.853860Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Global Mobile Net Model Fine Tuning","metadata":{}},{"cell_type":"code","source":"def create_Global_attention_augmented_mobilenet(input_shape, num_classes,attention=True):\n    # Load MobileNet base model without top layer\n    base_model = MobileNet(include_top=False, weights='imagenet', input_shape=input_shape)\n    base_model.trainable = False\n\n    # Attention augmented MobileNet architecture\n    \n    input_tensor = Input(shape=input_shape)\n    x = base_model(input_tensor)\n    # Apply attention module\n    if(attention):\n        x = Add()([attach_attention_module(x, attention_module='se_block'),x])\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)  # Applying ReLU activation after batch normalization\n\n    # Add classification layers\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    output_tensor = Dense(num_classes, activation='sigmoid')(x)\n\n    # Create the model\n    model = Model(inputs=input_tensor, outputs=output_tensor)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-10-11T12:58:08.029719Z","iopub.execute_input":"2023-10-11T12:58:08.030420Z","iopub.status.idle":"2023-10-11T12:58:08.036910Z","shell.execute_reply.started":"2023-10-11T12:58:08.030389Z","shell.execute_reply":"2023-10-11T12:58:08.035738Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Global InceptionV3 Model Fine Tuning","metadata":{}},{"cell_type":"code","source":"def create_Global_InceptionV3_model(input_shape, num_classes,attention=True):\n    \n    # Load Inception-v3 base model without top layer\n    base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=input_shape)\n    base_model.trainable = False\n\n    # Attention augmented Inception-v3 architecture\n    input_tensor = Input(shape=input_shape)\n    x = base_model(input_tensor)\n\n    # Apply attention module\n    if(attention):\n        x = Add()([attach_attention_module(x, attention_module='se_block'),x])\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)  # Applying ReLU activation after batch normalization\n\n\n    # Add classification layers\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    output_tensor = Dense(num_classes, activation='sigmoid')(x)\n\n    # Create the model\n    model = Model(inputs=input_tensor, outputs=output_tensor)\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T12:58:33.643299Z","iopub.execute_input":"2023-10-11T12:58:33.643692Z","iopub.status.idle":"2023-10-11T12:58:33.654299Z","shell.execute_reply.started":"2023-10-11T12:58:33.643661Z","shell.execute_reply":"2023-10-11T12:58:33.653411Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import classification_report, balanced_accuracy_score, average_precision_score, confusion_matrix\n\ndef evaluate_classification(y_true, y_pred, average='macro'):\n    \"\"\"\n    Evaluate the classification performance and calculate micro-average, balanced accuracy, and average precision.\n\n    Parameters:\n        y_true (numpy array or list): True labels.\n        y_pred (numpy array or list): Predicted labels.\n        average (str, optional): The averaging strategy to use for average precision.\n                                 Possible values are 'macro', 'micro', 'weighted', and None.\n                                 Default is 'macro'.\n\n    Returns:\n        report (str): The classification report as a string.\n        balanced_acc (float): The balanced accuracy.\n        avg_precision (float): The average precision.\n        micro_avg_precision (float): The micro-average precision.\n        micro_avg_recall (float): The micro-average recall.\n        micro_avg_f1_score (float): The micro-average F1-score.\n    \"\"\"\n    report = classification_report(y_true, y_pred, output_dict=True, zero_division=1)\n    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n    avg_precision = average_precision_score(y_true, y_pred, average=average)\n\n    # Calculate micro-average precision and recall using confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    tp_sum = np.sum(np.diag(cm))\n    pred_sum = np.sum(cm, axis=0)\n    true_sum = np.sum(cm, axis=1)\n    micro_avg_precision = tp_sum / pred_sum.sum()\n    micro_avg_recall = tp_sum / true_sum.sum()\n    micro_avg_f1_score = 2 * (micro_avg_precision * micro_avg_recall) / (micro_avg_precision + micro_avg_recall)\n\n    return report, balanced_acc, avg_precision, micro_avg_precision, micro_avg_recall, micro_avg_f1_score\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T12:58:37.840778Z","iopub.execute_input":"2023-10-11T12:58:37.841132Z","iopub.status.idle":"2023-10-11T12:58:37.848330Z","shell.execute_reply.started":"2023-10-11T12:58:37.841106Z","shell.execute_reply":"2023-10-11T12:58:37.847121Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# constant Declaration","metadata":{}},{"cell_type":"code","source":"# 1. Load and split the dataset\ntrain_data_dir = '/kaggle/input/cataract-image-dataset/processed_images/train'\n#validation_data_dir = 'd:/chaman/cataract/test'\ninput_shape = (224, 224)\nbatch_size = 32\nnum_classes=2\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T12:58:43.777912Z","iopub.execute_input":"2023-10-11T12:58:43.778238Z","iopub.status.idle":"2023-10-11T12:58:43.783108Z","shell.execute_reply.started":"2023-10-11T12:58:43.778212Z","shell.execute_reply":"2023-10-11T12:58:43.781971Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Traing and Validation + Data Augmentation Module","metadata":{}},{"cell_type":"code","source":"# 2. Preprocess the images\ntrain_datagen = ImageDataGenerator(\n    rescale=1.0/255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.2  # 20% validation split\n)\n\n#validation_datagen = ImageDataGenerator(rescale=1.0/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=input_shape,\n    batch_size=batch_size,\n    class_mode='categorical',  # Updated to 'categorical'\n    subset=\"training\"\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=input_shape,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'  # Subset for validation data\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T12:58:45.693402Z","iopub.execute_input":"2023-10-11T12:58:45.693727Z","iopub.status.idle":"2023-10-11T12:58:45.779452Z","shell.execute_reply.started":"2023-10-11T12:58:45.693700Z","shell.execute_reply":"2023-10-11T12:58:45.778637Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Found 393 images belonging to 2 classes.\nFound 98 images belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, EarlyStopping\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',  # Monitor validation loss\n    patience=40,          # Number of epochs with no improvement to wait\n    restore_best_weights=True,  # Restore the best model weights when stopping\n\n)\ncp_callback1 = tf.keras.callbacks.ModelCheckpoint('./fpseweights.h5', save_weights_only=True, verbose=1, save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T12:58:46.531251Z","iopub.execute_input":"2023-10-11T12:58:46.531939Z","iopub.status.idle":"2023-10-11T12:58:46.537099Z","shell.execute_reply.started":"2023-10-11T12:58:46.531900Z","shell.execute_reply":"2023-10-11T12:58:46.535840Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"cp_callback2 = tf.keras.callbacks.ModelCheckpoint('./fpseweights1.h5', save_weights_only=True, verbose=1, save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T12:58:47.488476Z","iopub.execute_input":"2023-10-11T12:58:47.489142Z","iopub.status.idle":"2023-10-11T12:58:47.494088Z","shell.execute_reply.started":"2023-10-11T12:58:47.489111Z","shell.execute_reply":"2023-10-11T12:58:47.493054Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"cp_callback3 = tf.keras.callbacks.ModelCheckpoint('./fpseweights2.h5', save_weights_only=True, verbose=1, save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T12:58:48.386780Z","iopub.execute_input":"2023-10-11T12:58:48.387622Z","iopub.status.idle":"2023-10-11T12:58:48.392548Z","shell.execute_reply.started":"2023-10-11T12:58:48.387582Z","shell.execute_reply":"2023-10-11T12:58:48.391932Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Local MobileNet Model Compilation","metadata":{}},{"cell_type":"code","source":"\nMobileNetModel = create_Global_attention_augmented_mobilenet(input_shape + (3,), num_classes,attention=True)\n\n# 4. Compile the model\nMobileNetModel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T12:58:50.113901Z","iopub.execute_input":"2023-10-11T12:58:50.114232Z","iopub.status.idle":"2023-10-11T12:58:55.199160Z","shell.execute_reply.started":"2023-10-11T12:58:50.114208Z","shell.execute_reply":"2023-10-11T12:58:55.198253Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n17225924/17225924 [==============================] - 1s 0us/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Local  InceptionV3 Model Compilation","metadata":{}},{"cell_type":"code","source":"InceptionV3Model = create_Global_InceptionV3_model(input_shape + (3,), num_classes)\n\nInceptionV3Model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T12:58:55.200983Z","iopub.execute_input":"2023-10-11T12:58:55.201510Z","iopub.status.idle":"2023-10-11T12:59:01.647755Z","shell.execute_reply.started":"2023-10-11T12:58:55.201482Z","shell.execute_reply":"2023-10-11T12:59:01.646818Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n87910968/87910968 [==============================] - 3s 0us/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Global Attention Augmented MobileNet Model Training","metadata":{}},{"cell_type":"code","source":"# 5. Train the model\nepochs = 100\n\n# Create a directory to save the best model weights during training\nos.makedirs('models', exist_ok=True)\nmodel_checkpoint = ModelCheckpoint('models/best_GlobalmobilnetmodelD1.h5', save_best_only=True, save_weights_only=True)\n\nhistory = MobileNetModel.fit(\n    train_generator,\n    steps_per_epoch=train_generator.n // train_generator.batch_size,\n    epochs=epochs,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.n // validation_generator.batch_size,\n    callbacks=[model_checkpoint,cp_callback1]\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T12:59:01.649137Z","iopub.execute_input":"2023-10-11T12:59:01.650048Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/100\n12/12 [==============================] - ETA: 0s - loss: 0.6785 - accuracy: 0.7292\nEpoch 1: val_loss improved from inf to 0.29813, saving model to ./fpseweights.h5\n12/12 [==============================] - 31s 2s/step - loss: 0.6785 - accuracy: 0.7292 - val_loss: 0.2981 - val_accuracy: 0.9271\nEpoch 2/100\n12/12 [==============================] - ETA: 0s - loss: 0.2215 - accuracy: 0.9224\nEpoch 2: val_loss did not improve from 0.29813\n12/12 [==============================] - 20s 2s/step - loss: 0.2215 - accuracy: 0.9224 - val_loss: 0.3336 - val_accuracy: 0.8958\nEpoch 3/100\n12/12 [==============================] - ETA: 0s - loss: 0.1410 - accuracy: 0.9529\nEpoch 3: val_loss improved from 0.29813 to 0.13783, saving model to ./fpseweights.h5\n12/12 [==============================] - 18s 2s/step - loss: 0.1410 - accuracy: 0.9529 - val_loss: 0.1378 - val_accuracy: 0.9688\nEpoch 4/100\n11/12 [==========================>...] - ETA: 1s - loss: 0.1346 - accuracy: 0.9514","output_type":"stream"}]},{"cell_type":"markdown","source":"# Global InceptionV3 Model Training","metadata":{}},{"cell_type":"code","source":"# Create a directory to save the best model weights during training\nos.makedirs('models', exist_ok=True)\nmodel_checkpoint = ModelCheckpoint('models/best_GlobalInceptionModelD1.h5', save_best_only=True, save_weights_only=True)\n\nInceptionV3history = InceptionV3Model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.n // train_generator.batch_size,\n    epochs=epochs,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.n // validation_generator.batch_size,\n    callbacks=[model_checkpoint,cp_callback2]\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MobileNetModel.save('GAAMD1.h5')\nInceptionV3Model.save('GAAIV3D1.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport numpy as np\nfrom keras.models import Model\nfrom keras.layers import Input\n#from keras.utils.generic_utils import get_custom_objects\nfrom keras.optimizers import Adam\n\ndef compute_gflops_and_model_size(model, input_shape=(1, 224, 224, 3)):\n    input_data = np.random.rand(*input_shape).astype(np.float32)\n    input_tensor = Input(shape=input_data.shape[1:])\n    model = Model(inputs=input_tensor, outputs=model(input_tensor))\n\n    macs = model.count_params()\n    params = np.sum([np.prod(w.shape) for w in model.get_weights()])\n\n    GFlops = macs * 2.0 / 10**9\n    model_size = params * 4.0 / 1024 / 1024  # Convert to MB\n    return GFlops, model_size\n\ndef compute_params(model):\n    return model.count_params()\n\ndef compute_fps(model, input_shape=(1, 224, 224, 3), epoch=100, device=None):\n    \"\"\"\n    Frames per second\n    :param input_shape: Input data shape\n    \"\"\"\n    if device:\n        model.compile(optimizer=Adam(), loss='mean_squared_error')  # Compile the model to initialize the weights\n        model.set_weights(model.get_weights())\n    \n    total_time = 0.0\n    data = np.random.rand(*input_shape).astype(np.float32)\n    \n    for i in range(epoch):\n        start = time.time()\n        predictions = model.predict(data, batch_size=input_shape[0])\n        end = time.time()\n        total_time += (end - start)\n\n    return total_time / epoch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import cifar10\n#from tensorflow.keras.applications import AlexNet\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndef compute_accuracy(y_true, y_pred, top_k=1):\n    top_k_categorical_accuracy = tf.keras.metrics.TopKCategoricalAccuracy(k=top_k)\n    top_k_categorical_accuracy.update_state(y_true, y_pred)\n    return top_k_categorical_accuracy.result().numpy() * 100.0\n\ndef compute_error(accuracy):\n    return 100.0 - accuracy\n\nif __name__ == '__main__':\n    # Load the CIFAR-10 dataset\n    #(x_test, y_test), _ = cifar10.load_data()\n    #x_test = x_test.astype(np.float32) / 255.0\n    #y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n\n    #model = AlexNet(weights='imagenet', input_shape=(224, 224, 3), include_top=True)\n\n    # Resize the images to (224, 224) and normalize\n    #datagen = ImageDataGenerator(\n    #    preprocessing_function=lambda x: (x - 0.5) / 0.5,\n    #    width_shift_range=0.1,\n    #    height_shift_range=0.1,\n    #    fill_mode='reflect'\n    #)\n\n    #data_generator = datagen.flow(x_test, y_test, batch_size=128)\n\n    # Calculate top-1 and top-5 accuracy\n    top1_accuracy = compute_accuracy(y_test, model.predict(x_test), top_k=1)\n    #top5_accuracy = compute_accuracy(y_test, model.predict(x_test), top_k=5)\n\n    # Calculate top-1 and top-5 error\n    top1_error = compute_error(top1_accuracy)\n    #top5_error = compute_error(top5_accuracy)\n\n    print(f'Top 1 Accuracy: {top1_accuracy:.3f}%')\n    print(f'Top 5 Accuracy: {top5_accuracy:.3f}%')\n    print(f'Top 1 Error: {top1_error:.3f}%')\n    print(f'Top 5 Error: {top5_error:.3f}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'InceptionV3: GFlops and Model Size: {compute_gflops_and_model_size(InceptionV3Model)}')\nprint(f'InceptionV3:Parameter Counting: {compute_params(InceptionV3Model)}')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'MobileNet: GFlops and Model Size: {compute_gflops_and_model_size(MobileNetModel)}')\nprint(f'MobileNet:Parameter Counting: {compute_params(MobileNetModel)}')\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T09:52:54.350196Z","iopub.execute_input":"2023-10-11T09:52:54.351281Z","iopub.status.idle":"2023-10-11T09:52:54.631666Z","shell.execute_reply.started":"2023-10-11T09:52:54.351242Z","shell.execute_reply":"2023-10-11T09:52:54.630744Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"MobileNet: GFlops and Model Size: (0.009089668, 17.33716583251953)\nMobileNet:Parameter Counting: 4544834\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f'MobileNet FPS: {compute_fps(MobileNetModel)}')","metadata":{"execution":{"iopub.status.busy":"2023-10-11T09:53:13.048028Z","iopub.execute_input":"2023-10-11T09:53:13.048351Z","iopub.status.idle":"2023-10-11T09:53:18.395675Z","shell.execute_reply.started":"2023-10-11T09:53:13.048326Z","shell.execute_reply":"2023-10-11T09:53:18.394638Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 1s 568ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 20ms/step\nMobileNet FPS: 0.05340376377105713\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Global MobileNet Model Validation Evaluation","metadata":{}},{"cell_type":"code","source":"print(f'InceptionV3 FPS: {compute_fps(InceptionV3Model)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 6. Evaluate the model\nMobileNetEvaluation = MobileNetModel.evaluate(validation_generator)\nprint(\"Validation Accuracy: {:.2f}%\".format(MobileNetEvaluation[1] * 100))","metadata":{"execution":{"iopub.status.busy":"2023-10-11T12:45:15.708212Z","iopub.execute_input":"2023-10-11T12:45:15.709243Z","iopub.status.idle":"2023-10-11T12:45:18.731217Z","shell.execute_reply.started":"2023-10-11T12:45:15.709208Z","shell.execute_reply":"2023-10-11T12:45:18.730209Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"4/4 [==============================] - 2s 511ms/step - loss: 0.0655 - accuracy: 0.9796\nValidation Accuracy: 97.96%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Global InceptionV3 Model Validation Evaluation","metadata":{}},{"cell_type":"code","source":"# 6. Evaluate the model\nInceptionV3Evaluation = InceptionV3Model.evaluate(validation_generator)\nprint(\"Validation Accuracy: {:.2f}%\".format(InceptionV3Evaluation[1] * 100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Common TestGenerator Declaration Module","metadata":{}},{"cell_type":"code","source":"# Test data directory\ntest_data_dir = '/kaggle/input/cataract-image-dataset/processed_images/test'\ninput_shape = (224, 224)\nbatch_size = 32\n\n# Preprocess the test images\ntest_datagen = ImageDataGenerator(rescale=1.0/255)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_data_dir,\n    target_size=input_shape,\n    batch_size=batch_size,\n    class_mode='categorical',  # Set to 'categorical' if you used the updated code\n    shuffle=False\n)\n\n# Load the saved model\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T11:32:13.469145Z","iopub.execute_input":"2023-10-11T11:32:13.469480Z","iopub.status.idle":"2023-10-11T11:32:13.483184Z","shell.execute_reply.started":"2023-10-11T11:32:13.469454Z","shell.execute_reply":"2023-10-11T11:32:13.482490Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Found 121 images belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Global MobileNet Model Test Evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluate the model on the test set\nMobileNetTestEvaluation = MobileNetModel.evaluate_generator(test_generator)\nprint(\"Test Accuracy: {:.2f}%\".format(MobileNetTestEvaluation[1] * 100))","metadata":{"execution":{"iopub.status.busy":"2023-10-11T12:45:32.368694Z","iopub.execute_input":"2023-10-11T12:45:32.369336Z","iopub.status.idle":"2023-10-11T12:45:37.577733Z","shell.execute_reply.started":"2023-10-11T12:45:32.369307Z","shell.execute_reply":"2023-10-11T12:45:37.576709Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_70/4230226302.py:2: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n  MobileNetTestEvaluation = MobileNetModel.evaluate_generator(test_generator)\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 95.87%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Global InceptionV3 Model Test Evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluate the model on the test set\nInceptionV3TestEvaluation = InceptionV3Model.evaluate_generator(test_generator)\nprint(\"Test Accuracy: {:.2f}%\".format(InceptionV3TestEvaluation[1] * 100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Common Classification Report Module","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Load the saved model\n#model = tf.keras.models.load_model('models/final_model.h5')  # Update with the correct path\n\n# Get the true labels for the test set\ntrue_labels = test_generator.classes\n\n# Compute and print the classification report\nclass_names = list(test_generator.class_indices.keys())\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T10:33:30.413708Z","iopub.execute_input":"2023-10-11T10:33:30.414362Z","iopub.status.idle":"2023-10-11T10:33:30.419033Z","shell.execute_reply.started":"2023-10-11T10:33:30.414332Z","shell.execute_reply":"2023-10-11T10:33:30.417833Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# Global MobileNet Model Classification Report","metadata":{}},{"cell_type":"code","source":"# Generate predictions for the test set\nMobileNetPredictions = MobileNetModel.predict_generator(test_generator)\nMobileNetPredicted_labels = np.argmax(MobileNetPredictions, axis=1)\nMobileNetReport = classification_report(true_labels, MobileNetPredicted_labels, target_names=class_names)\nprint(MobileNetReport)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T10:33:41.552372Z","iopub.execute_input":"2023-10-11T10:33:41.552701Z","iopub.status.idle":"2023-10-11T10:33:47.174260Z","shell.execute_reply.started":"2023-10-11T10:33:41.552674Z","shell.execute_reply":"2023-10-11T10:33:47.173338Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_70/2479908967.py:2: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n  MobileNetPredictions = MobileNetModel.predict_generator(test_generator)\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n    cataract       0.94      0.97      0.95        61\n      normal       0.97      0.93      0.95        60\n\n    accuracy                           0.95       121\n   macro avg       0.95      0.95      0.95       121\nweighted avg       0.95      0.95      0.95       121\n\n","output_type":"stream"}]},{"cell_type":"code","source":"report, balanced_acc, avg_precision, micro_avg_precision, micro_avg_recall, micro_avg_f1_score = evaluate_classification(true_labels,  MobileNetPredicted_labels)\n\n# Print the classification report and additional metrics\n#print(\"Classification Report:\\n\", report)\nprint(\"Micro-average F1-score:\", micro_avg_f1_score)\nprint(\"Balanced Accuracy:\", balanced_acc)\nprint(\"Average Precision:\", avg_precision)\nprint(\"Micro-average Precision:\", micro_avg_precision)\nprint(\"Micro-average Recall:\", micro_avg_recall)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Global InceptionV3 Model Classification Report","metadata":{}},{"cell_type":"code","source":"# Generate predictions for the test set\nInceptionV3Predictions = InceptionV3Model.predict_generator(test_generator)\nInceptionV3Predicted_labels = np.argmax(InceptionV3Predictions, axis=1)\nInceptionV3Report = classification_report(true_labels, InceptionV3Predicted_labels, target_names=class_names)\nprint(InceptionV3Report)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report, balanced_acc, avg_precision, micro_avg_precision, micro_avg_recall, micro_avg_f1_score = evaluate_classification(true_labels,  InceptionV3Predicted_labels)\n\n# Print the classification report and additional metrics\n#print(\"Classification Report:\\n\", report)\nprint(\"Micro-average F1-score:\", micro_avg_f1_score)\nprint(\"Balanced Accuracy:\", balanced_acc)\nprint(\"Average Precision:\", avg_precision)\nprint(\"Micro-average Precision:\", micro_avg_precision)\nprint(\"Micro-average Recall:\", micro_avg_recall)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Global MobileNet Model Confusion Matrix","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nfrom mlxtend.plotting import plot_confusion_matrix\n\n\n# Load the saved model\n#model = tf.keras.models.load_model('models/final_model.h5')  # Update with the correct path\n\n# Generate predictions for the test data\n\n\n# Generate the confusion matrix\nMobileNetCM = confusion_matrix(true_labels, MobileNetPredicted_labels)\n\n# Plot confusion matrix\nplt.figure(figsize=(5, 3))\nclass_names = list(test_generator.class_indices.keys())\n#sns.heatmap(MobileNetCM, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\nplot_confusion_matrix(conf_mat=MobileNetCM, figsize=(5, 3), class_names=['Cataract', 'Normal'], show_normed=True)\nplt.title('GAAMD1 Confusion Matrix')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.tight_layout()\n#plt.savefig('GAAMD1.eps',dpi=250)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Global Inceptionv3 Model Confusion Matrix","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Load the saved model\n#model = tf.keras.models.load_model('models/final_model.h5')  # Update with the correct path\n\n# Generate predictions for the test data\n\n\n# Generate the confusion matrix\nInceptionV3CM = confusion_matrix(true_labels, InceptionV3Predicted_labels)\n\n# Plot confusion matrix\nplt.figure(figsize=(5, 3))\nclass_names = list(test_generator.class_indices.keys())\n#sns.heatmap(InceptionV3CM, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\nplot_confusion_matrix(conf_mat=InceptionV3CM, figsize=(5, 3), class_names=['Cataract', 'Normal'], show_normed=True)\nplt.title('GAAIV3D1 Confusion Matrix')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.tight_layout()\n#plt.savefig('GAAIV3D1.eps',dpi=250)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\n\nfpr_model1, tpr_model1, thresholds_model1 = roc_curve(true_labels, MobileNetPredicted_labels)\nauc_model1 = roc_auc_score(true_labels, MobileNetPredicted_labels)\n\n# Calculate ROC curve and AUC for Model 2\nfpr_model2, tpr_model2, thresholds_model2 = roc_curve(true_labels, InceptionV3Predicted_labels)\nauc_model2 = roc_auc_score(true_labels, InceptionV3Predicted_labels)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot ROC curves\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_model1, tpr_model1, label='GlobalMobileNetD1 (AUC = {:.2f})'.format(auc_model1))\nplt.plot(fpr_model2, tpr_model2, label='GlobalInceptionV3D1 (AUC = {:.2f})'.format(auc_model2))\nplt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('GAAMD1 and GAAIV2D1 ROC Curve Comparison')\nplt.legend(loc='lower right')\nplt.grid(True)\n#plt.savefig('GAAMvsGAAIRoc.eps',dpi=250)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n\n# Assuming you have predictions and true labels for both models\n# Replace 'MobileNetPredicted_labels' and 'InceptionV3Predicted_labels' with the predictions of your models\n# Replace 'true_labels' with the true labels of your data\n\n# Calculate precision-recall curve for Model 1\nprecision_model1, recall_model1, thresholds_model1 = precision_recall_curve(true_labels, MobileNetPredicted_labels)\n\n# Calculate precision-recall curve for Model 2\nprecision_model2, recall_model2, thresholds_model2 = precision_recall_curve(true_labels, InceptionV3Predicted_labels)\n\nimport matplotlib.pyplot as plt\n\n# Plot the precision-recall curves for both models\nplt.figure(figsize=(8, 6))\nplt.plot(recall_model1, precision_model1, marker='.', label='GAAMD1')\nplt.plot(recall_model2, precision_model2, marker='.', label='GAAIV3D1')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve Comparison')\nplt.legend()\n\n# Annotate points with precision-recall values for Model 1\nfor p, r, t in zip(precision_model1, recall_model1, thresholds_model1):\n    plt.annotate(f'LGMobileNet: {t:.2f}\\n{p:.2f}/{r:.2f}', xy=(r, p), xytext=(r + 0.03, p), arrowprops=dict(arrowstyle='->'), fontsize=8)\n\n# Annotate points with precision-recall values for Model 2\nfor p, r, t in zip(precision_model2, recall_model2, thresholds_model2):\n    plt.annotate(f'LGInceptionV3: {t:.2f}\\n{p:.2f}/{r:.2f}', xy=(r, p), xytext=(r - 0.15, p), arrowprops=dict(arrowstyle='->'), fontsize=8)\n\nplt.grid(True)\n#plt.savefig('GAAMvsGAAID1PR.eps',dpi=250)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MobileNetModel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport random\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\n# Step 1: Load and preprocess the image data (Replace 'your_image_path' with the image you want to visualize)\n# Step 1: Load and preprocess the image data\ntest_folder = '/kaggle/input/cataract-image-dataset/processed_images/test'\ninput_size = (224, 224)  # Input size of the model, adjust according to your model's input size\n# Step 2: Pick 4 sample images (2 from 'cataract' class and 2 from 'normal' class)\nclass_folders = ['cataract', 'normal']\nsample_images = []\n\nfor class_folder in class_folders:\n    class_path = os.path.join(test_folder, class_folder)\n    images = os.listdir(class_path)\n    sample_images.extend(random.sample(images, 2))\n\ndef preprocess_image(image_path, input_size):\n    img = load_img(image_path, target_size=input_size)\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = tf.keras.applications.vgg16.preprocess_input(img)  # Preprocess based on VGG16 requirements\n    return img\n\n\n# Step 3: Define a function to compute the Grad-CAM visualization\ndef compute_gradcam(model, image):\n    # Get the last convolutional layer and the output layer of the model\n    last_conv_layer = model.get_layer('multiply')\n    output_layer = model.layers[-1]\n\n    # Create a model that maps the input image to the output class predictions and the last convolutional layer\n    grad_model = Model(inputs=model.input, outputs=[last_conv_layer.output, output_layer.output])\n\n    # Compute the gradients of the predicted class with respect to the last convolutional layer\n    with tf.GradientTape() as tape:\n        conv_output, predictions = grad_model(image)\n        loss = predictions[:, 0]  # Assuming binary classification, change this if you have different output classes\n\n    grads = tape.gradient(loss, conv_output)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # Multiply each channel in the feature map by its corresponding gradient importance\n    conv_output = conv_output[0]\n    heatmap = tf.reduce_sum(tf.multiply(conv_output, pooled_grads), axis=-1)\n    heatmap = np.maximum(heatmap, 0)  # Apply ReLU activation\n    heatmap /= np.max(heatmap)  # Normalize the heatmap values between 0 and 1\n\n    return heatmap\n\n\n# Create a figure with subplots for original images and their Grad-CAM images\nnum_images = len(sample_images)\nfig, axes = plt.subplots(2, num_images, figsize=(4*num_images, 8))\n\n# Generate Grad-CAM for each image and plot the results\nfor i, image_name in enumerate(sample_images):\n    class_folder = 'cataract' if 'cataract' in image_name else 'normal'\n    image_path = os.path.join(test_folder, class_folder, image_name)\n    image = preprocess_image(image_path, input_size)\n    heatmap = compute_gradcam(MobileNetModel, image)\n    heatmap = cv2.resize(heatmap, (input_size[1], input_size[0]))\n    heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n\n    # Load the original image\n    original_image = cv2.imread(image_path)\n    original_image = cv2.resize(original_image, (input_size[1], input_size[0]))\n\n    # Overlay the heatmap on the original image\n    alpha = 0.5  # Adjust the alpha value for the heatmap overlay\n    superimposed_img = cv2.addWeighted(original_image, alpha, heatmap, 1 - alpha, 0)\n\n    # Display the original image and Grad-CAM side by side\n    axes[0, i].imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n    axes[0, i].set_title('Original Image')\n    axes[0, i].axis('off')\n\n    axes[1, i].imshow(cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB))\n    axes[1, i].set_title('Grad-CAM Visualization')\n    axes[1, i].axis('off')\n\nplt.tight_layout()\nplt.savefig('LGAAM_D2_GradCamVisualization.eps',dpi=300)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the Grad-CAM heatmap:\n\nRegions with warm colors (e.g., red, orange, and yellow) indicate high importance or high activation. \nThese regions are crucial for the model's prediction, and the model relies heavily on the features extracted \nfrom these areas to make its decision.\n\nRegions with cool colors (e.g., blue and green) indicate low importance or low activation. \nThese regions are less relevant to the model's prediction, and the model does not rely much on \nthe features from these areas to make its decision.\n\nIn a binary classification scenario like this, where the model predicts between 'cataract' and 'normal' \nclasses, the heatmap will show which parts of the image the model considers important for classifying \nthe input as 'cataract' (warm regions) and which parts are not as relevant for the 'normal' class (cool regions).","metadata":{}},{"cell_type":"code","source":"MobileNetModelWA = create_Global_attention_augmented_mobilenet(input_shape + (3,), num_classes,attention=False)\n\n# 4. Compile the model\nMobileNetModelWA.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T09:54:19.812708Z","iopub.execute_input":"2023-10-11T09:54:19.813425Z","iopub.status.idle":"2023-10-11T09:54:20.608029Z","shell.execute_reply.started":"2023-10-11T09:54:19.813395Z","shell.execute_reply":"2023-10-11T09:54:20.607090Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"InceptionV3ModelWA = create_Global_InceptionV3_model(input_shape + (3,), num_classes,attention=False)\n\nInceptionV3ModelWA.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T09:54:23.102151Z","iopub.execute_input":"2023-10-11T09:54:23.102838Z","iopub.status.idle":"2023-10-11T09:54:25.742731Z","shell.execute_reply.started":"2023-10-11T09:54:23.102800Z","shell.execute_reply":"2023-10-11T09:54:25.741813Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# 5. Train the model\nepochs = 100\n\n# Create a directory to save the best model weights during training\nos.makedirs('models', exist_ok=True)\nmodel_checkpoint = ModelCheckpoint('models/best_GlobalmobilnetmodelD1.h5', save_best_only=True, save_weights_only=True)\n\nhistory = MobileNetModelWA.fit(\n    train_generator,\n    steps_per_epoch=train_generator.n // train_generator.batch_size,\n    epochs=epochs,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.n // validation_generator.batch_size,\n    callbacks=[model_checkpoint,cp_callback3]\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T09:54:34.865331Z","iopub.execute_input":"2023-10-11T09:54:34.865982Z","iopub.status.idle":"2023-10-11T10:26:44.135662Z","shell.execute_reply.started":"2023-10-11T09:54:34.865953Z","shell.execute_reply":"2023-10-11T10:26:44.134651Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Epoch 1/100\n12/12 [==============================] - ETA: 0s - loss: 1.5452 - accuracy: 0.6787\nEpoch 1: val_loss improved from inf to 0.42885, saving model to ./fpseweights2.h5\n12/12 [==============================] - 20s 2s/step - loss: 1.5452 - accuracy: 0.6787 - val_loss: 0.4288 - val_accuracy: 0.8542\nEpoch 2/100\n12/12 [==============================] - ETA: 0s - loss: 0.2967 - accuracy: 0.8892\nEpoch 2: val_loss improved from 0.42885 to 0.22813, saving model to ./fpseweights2.h5\n12/12 [==============================] - 19s 2s/step - loss: 0.2967 - accuracy: 0.8892 - val_loss: 0.2281 - val_accuracy: 0.9167\nEpoch 3/100\n12/12 [==============================] - ETA: 0s - loss: 0.1934 - accuracy: 0.9252\nEpoch 3: val_loss improved from 0.22813 to 0.18232, saving model to ./fpseweights2.h5\n12/12 [==============================] - 19s 2s/step - loss: 0.1934 - accuracy: 0.9252 - val_loss: 0.1823 - val_accuracy: 0.9271\nEpoch 4/100\n12/12 [==============================] - ETA: 0s - loss: 0.1598 - accuracy: 0.9529\nEpoch 4: val_loss improved from 0.18232 to 0.15171, saving model to ./fpseweights2.h5\n12/12 [==============================] - 20s 2s/step - loss: 0.1598 - accuracy: 0.9529 - val_loss: 0.1517 - val_accuracy: 0.9583\nEpoch 5/100\n12/12 [==============================] - ETA: 0s - loss: 0.1291 - accuracy: 0.9557\nEpoch 5: val_loss improved from 0.15171 to 0.12042, saving model to ./fpseweights2.h5\n12/12 [==============================] - 18s 2s/step - loss: 0.1291 - accuracy: 0.9557 - val_loss: 0.1204 - val_accuracy: 0.9583\nEpoch 6/100\n12/12 [==============================] - ETA: 0s - loss: 0.1212 - accuracy: 0.9557\nEpoch 6: val_loss improved from 0.12042 to 0.11760, saving model to ./fpseweights2.h5\n12/12 [==============================] - 17s 2s/step - loss: 0.1212 - accuracy: 0.9557 - val_loss: 0.1176 - val_accuracy: 0.9375\nEpoch 7/100\n12/12 [==============================] - ETA: 0s - loss: 0.0937 - accuracy: 0.9695\nEpoch 7: val_loss improved from 0.11760 to 0.10863, saving model to ./fpseweights2.h5\n12/12 [==============================] - 18s 2s/step - loss: 0.0937 - accuracy: 0.9695 - val_loss: 0.1086 - val_accuracy: 0.9688\nEpoch 8/100\n12/12 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9723\nEpoch 8: val_loss improved from 0.10863 to 0.09353, saving model to ./fpseweights2.h5\n12/12 [==============================] - 17s 1s/step - loss: 0.0797 - accuracy: 0.9723 - val_loss: 0.0935 - val_accuracy: 0.9688\nEpoch 9/100\n12/12 [==============================] - ETA: 0s - loss: 0.0829 - accuracy: 0.9751\nEpoch 9: val_loss improved from 0.09353 to 0.06933, saving model to ./fpseweights2.h5\n12/12 [==============================] - 18s 2s/step - loss: 0.0829 - accuracy: 0.9751 - val_loss: 0.0693 - val_accuracy: 0.9896\nEpoch 10/100\n12/12 [==============================] - ETA: 0s - loss: 0.0823 - accuracy: 0.9778\nEpoch 10: val_loss did not improve from 0.06933\n12/12 [==============================] - 17s 1s/step - loss: 0.0823 - accuracy: 0.9778 - val_loss: 0.1220 - val_accuracy: 0.9583\nEpoch 11/100\n12/12 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9834\nEpoch 11: val_loss improved from 0.06933 to 0.05953, saving model to ./fpseweights2.h5\n12/12 [==============================] - 20s 2s/step - loss: 0.0802 - accuracy: 0.9834 - val_loss: 0.0595 - val_accuracy: 0.9792\nEpoch 12/100\n12/12 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9834\nEpoch 12: val_loss did not improve from 0.05953\n12/12 [==============================] - 17s 1s/step - loss: 0.0664 - accuracy: 0.9834 - val_loss: 0.0649 - val_accuracy: 0.9792\nEpoch 13/100\n12/12 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9806\nEpoch 13: val_loss did not improve from 0.05953\n12/12 [==============================] - 17s 2s/step - loss: 0.0618 - accuracy: 0.9806 - val_loss: 0.0896 - val_accuracy: 0.9688\nEpoch 14/100\n12/12 [==============================] - ETA: 0s - loss: 0.0629 - accuracy: 0.9723\nEpoch 14: val_loss did not improve from 0.05953\n12/12 [==============================] - 18s 1s/step - loss: 0.0629 - accuracy: 0.9723 - val_loss: 0.0694 - val_accuracy: 0.9688\nEpoch 15/100\n12/12 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9861\nEpoch 15: val_loss did not improve from 0.05953\n12/12 [==============================] - 18s 1s/step - loss: 0.0457 - accuracy: 0.9861 - val_loss: 0.1058 - val_accuracy: 0.9688\nEpoch 16/100\n12/12 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9778\nEpoch 16: val_loss did not improve from 0.05953\n12/12 [==============================] - 17s 2s/step - loss: 0.0670 - accuracy: 0.9778 - val_loss: 0.0657 - val_accuracy: 0.9792\nEpoch 17/100\n12/12 [==============================] - ETA: 0s - loss: 0.0443 - accuracy: 0.9889\nEpoch 17: val_loss did not improve from 0.05953\n12/12 [==============================] - 20s 2s/step - loss: 0.0443 - accuracy: 0.9889 - val_loss: 0.0981 - val_accuracy: 0.9583\nEpoch 18/100\n12/12 [==============================] - ETA: 0s - loss: 0.0642 - accuracy: 0.9792\nEpoch 18: val_loss did not improve from 0.05953\n12/12 [==============================] - 20s 2s/step - loss: 0.0642 - accuracy: 0.9792 - val_loss: 0.0655 - val_accuracy: 0.9792\nEpoch 19/100\n12/12 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9751\nEpoch 19: val_loss did not improve from 0.05953\n12/12 [==============================] - 18s 1s/step - loss: 0.0681 - accuracy: 0.9751 - val_loss: 0.0663 - val_accuracy: 0.9792\nEpoch 20/100\n12/12 [==============================] - ETA: 0s - loss: 0.0444 - accuracy: 0.9889\nEpoch 20: val_loss did not improve from 0.05953\n12/12 [==============================] - 17s 1s/step - loss: 0.0444 - accuracy: 0.9889 - val_loss: 0.0728 - val_accuracy: 0.9792\nEpoch 21/100\n12/12 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9834\nEpoch 21: val_loss did not improve from 0.05953\n12/12 [==============================] - 17s 2s/step - loss: 0.0485 - accuracy: 0.9834 - val_loss: 0.0849 - val_accuracy: 0.9583\nEpoch 22/100\n12/12 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.9834\nEpoch 22: val_loss did not improve from 0.05953\n12/12 [==============================] - 19s 2s/step - loss: 0.0451 - accuracy: 0.9834 - val_loss: 0.1039 - val_accuracy: 0.9375\nEpoch 23/100\n12/12 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 0.9834\nEpoch 23: val_loss did not improve from 0.05953\n12/12 [==============================] - 17s 1s/step - loss: 0.0390 - accuracy: 0.9834 - val_loss: 0.0735 - val_accuracy: 0.9792\nEpoch 24/100\n12/12 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9889\nEpoch 24: val_loss improved from 0.05953 to 0.04708, saving model to ./fpseweights2.h5\n12/12 [==============================] - 19s 2s/step - loss: 0.0289 - accuracy: 0.9889 - val_loss: 0.0471 - val_accuracy: 0.9896\nEpoch 25/100\n12/12 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9889\nEpoch 25: val_loss improved from 0.04708 to 0.03869, saving model to ./fpseweights2.h5\n12/12 [==============================] - 17s 1s/step - loss: 0.0350 - accuracy: 0.9889 - val_loss: 0.0387 - val_accuracy: 0.9896\nEpoch 26/100\n12/12 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9889\nEpoch 26: val_loss did not improve from 0.03869\n12/12 [==============================] - 19s 2s/step - loss: 0.0340 - accuracy: 0.9889 - val_loss: 0.0918 - val_accuracy: 0.9583\nEpoch 27/100\n12/12 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9972\nEpoch 27: val_loss improved from 0.03869 to 0.02310, saving model to ./fpseweights2.h5\n12/12 [==============================] - 20s 2s/step - loss: 0.0173 - accuracy: 0.9972 - val_loss: 0.0231 - val_accuracy: 0.9896\nEpoch 28/100\n12/12 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9972\nEpoch 28: val_loss improved from 0.02310 to 0.01642, saving model to ./fpseweights2.h5\n12/12 [==============================] - 19s 2s/step - loss: 0.0116 - accuracy: 0.9972 - val_loss: 0.0164 - val_accuracy: 1.0000\nEpoch 29/100\n12/12 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9917\nEpoch 29: val_loss did not improve from 0.01642\n12/12 [==============================] - 18s 1s/step - loss: 0.0310 - accuracy: 0.9917 - val_loss: 0.0237 - val_accuracy: 0.9896\nEpoch 30/100\n12/12 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9972\nEpoch 30: val_loss did not improve from 0.01642\n12/12 [==============================] - 19s 2s/step - loss: 0.0170 - accuracy: 0.9972 - val_loss: 0.0588 - val_accuracy: 0.9896\nEpoch 31/100\n12/12 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9945\nEpoch 31: val_loss did not improve from 0.01642\n12/12 [==============================] - 17s 1s/step - loss: 0.0183 - accuracy: 0.9945 - val_loss: 0.0302 - val_accuracy: 0.9896\nEpoch 32/100\n12/12 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9917\nEpoch 32: val_loss did not improve from 0.01642\n12/12 [==============================] - 20s 2s/step - loss: 0.0279 - accuracy: 0.9917 - val_loss: 0.0309 - val_accuracy: 0.9792\nEpoch 33/100\n12/12 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9945\nEpoch 33: val_loss did not improve from 0.01642\n12/12 [==============================] - 17s 1s/step - loss: 0.0144 - accuracy: 0.9945 - val_loss: 0.0500 - val_accuracy: 0.9688\nEpoch 34/100\n12/12 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9945\nEpoch 34: val_loss did not improve from 0.01642\n12/12 [==============================] - 19s 2s/step - loss: 0.0246 - accuracy: 0.9945 - val_loss: 0.0339 - val_accuracy: 0.9896\nEpoch 35/100\n12/12 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9972\nEpoch 35: val_loss did not improve from 0.01642\n12/12 [==============================] - 19s 2s/step - loss: 0.0172 - accuracy: 0.9972 - val_loss: 0.0353 - val_accuracy: 0.9896\nEpoch 36/100\n12/12 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9917\nEpoch 36: val_loss did not improve from 0.01642\n12/12 [==============================] - 19s 2s/step - loss: 0.0261 - accuracy: 0.9917 - val_loss: 0.0265 - val_accuracy: 0.9792\nEpoch 37/100\n12/12 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9917\nEpoch 37: val_loss did not improve from 0.01642\n12/12 [==============================] - 20s 2s/step - loss: 0.0264 - accuracy: 0.9917 - val_loss: 0.0443 - val_accuracy: 0.9792\nEpoch 38/100\n12/12 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9972\nEpoch 38: val_loss did not improve from 0.01642\n12/12 [==============================] - 17s 1s/step - loss: 0.0151 - accuracy: 0.9972 - val_loss: 0.0356 - val_accuracy: 0.9896\nEpoch 39/100\n12/12 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9889\nEpoch 39: val_loss did not improve from 0.01642\n12/12 [==============================] - 20s 2s/step - loss: 0.0285 - accuracy: 0.9889 - val_loss: 0.0462 - val_accuracy: 0.9896\nEpoch 40/100\n12/12 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 0.9870\nEpoch 40: val_loss did not improve from 0.01642\n12/12 [==============================] - 18s 2s/step - loss: 0.0339 - accuracy: 0.9870 - val_loss: 0.0879 - val_accuracy: 0.9792\nEpoch 41/100\n12/12 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9972\nEpoch 41: val_loss did not improve from 0.01642\n12/12 [==============================] - 19s 2s/step - loss: 0.0206 - accuracy: 0.9972 - val_loss: 0.0258 - val_accuracy: 0.9896\nEpoch 42/100\n12/12 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9917\nEpoch 42: val_loss did not improve from 0.01642\n12/12 [==============================] - 18s 2s/step - loss: 0.0302 - accuracy: 0.9917 - val_loss: 0.0883 - val_accuracy: 0.9375\nEpoch 43/100\n12/12 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000\nEpoch 43: val_loss did not improve from 0.01642\n12/12 [==============================] - 20s 2s/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.1265 - val_accuracy: 0.9583\nEpoch 44/100\n12/12 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9917\nEpoch 44: val_loss did not improve from 0.01642\n12/12 [==============================] - 17s 1s/step - loss: 0.0422 - accuracy: 0.9917 - val_loss: 0.0574 - val_accuracy: 0.9688\nEpoch 45/100\n12/12 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9917\nEpoch 45: val_loss did not improve from 0.01642\n12/12 [==============================] - 20s 2s/step - loss: 0.0196 - accuracy: 0.9917 - val_loss: 0.0403 - val_accuracy: 0.9896\nEpoch 46/100\n12/12 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9945\nEpoch 46: val_loss improved from 0.01642 to 0.01213, saving model to ./fpseweights2.h5\n12/12 [==============================] - 17s 1s/step - loss: 0.0150 - accuracy: 0.9945 - val_loss: 0.0121 - val_accuracy: 1.0000\nEpoch 47/100\n12/12 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9972\nEpoch 47: val_loss did not improve from 0.01213\n12/12 [==============================] - 20s 2s/step - loss: 0.0142 - accuracy: 0.9972 - val_loss: 0.0429 - val_accuracy: 0.9792\nEpoch 48/100\n12/12 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9945\nEpoch 48: val_loss did not improve from 0.01213\n12/12 [==============================] - 19s 2s/step - loss: 0.0246 - accuracy: 0.9945 - val_loss: 0.0481 - val_accuracy: 0.9688\nEpoch 49/100\n12/12 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.9972\nEpoch 49: val_loss did not improve from 0.01213\n12/12 [==============================] - 19s 2s/step - loss: 0.0095 - accuracy: 0.9972 - val_loss: 0.0143 - val_accuracy: 1.0000\nEpoch 50/100\n12/12 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9945\nEpoch 50: val_loss did not improve from 0.01213\n12/12 [==============================] - 20s 2s/step - loss: 0.0108 - accuracy: 0.9945 - val_loss: 0.0402 - val_accuracy: 0.9792\nEpoch 51/100\n12/12 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 1.0000\nEpoch 51: val_loss did not improve from 0.01213\n12/12 [==============================] - 20s 2s/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0839 - val_accuracy: 0.9688\nEpoch 52/100\n12/12 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9972\nEpoch 52: val_loss did not improve from 0.01213\n12/12 [==============================] - 18s 1s/step - loss: 0.0160 - accuracy: 0.9972 - val_loss: 0.0476 - val_accuracy: 0.9688\nEpoch 53/100\n12/12 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9972\nEpoch 53: val_loss did not improve from 0.01213\n12/12 [==============================] - 20s 2s/step - loss: 0.0101 - accuracy: 0.9972 - val_loss: 0.0675 - val_accuracy: 0.9792\nEpoch 54/100\n12/12 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9972\nEpoch 54: val_loss did not improve from 0.01213\n12/12 [==============================] - 17s 1s/step - loss: 0.0199 - accuracy: 0.9972 - val_loss: 0.0395 - val_accuracy: 0.9896\nEpoch 55/100\n12/12 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9945\nEpoch 55: val_loss did not improve from 0.01213\n12/12 [==============================] - 20s 2s/step - loss: 0.0176 - accuracy: 0.9945 - val_loss: 0.0707 - val_accuracy: 0.9688\nEpoch 56/100\n12/12 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.9945\nEpoch 56: val_loss did not improve from 0.01213\n12/12 [==============================] - 19s 2s/step - loss: 0.0102 - accuracy: 0.9945 - val_loss: 0.0429 - val_accuracy: 0.9792\nEpoch 57/100\n12/12 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9972\nEpoch 57: val_loss did not improve from 0.01213\n12/12 [==============================] - 19s 2s/step - loss: 0.0126 - accuracy: 0.9972 - val_loss: 0.0751 - val_accuracy: 0.9688\nEpoch 58/100\n12/12 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.9917\nEpoch 58: val_loss did not improve from 0.01213\n12/12 [==============================] - 17s 1s/step - loss: 0.0305 - accuracy: 0.9917 - val_loss: 0.0535 - val_accuracy: 0.9688\nEpoch 59/100\n12/12 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9917\nEpoch 59: val_loss did not improve from 0.01213\n12/12 [==============================] - 19s 2s/step - loss: 0.0244 - accuracy: 0.9917 - val_loss: 0.1377 - val_accuracy: 0.9583\nEpoch 60/100\n12/12 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9889\nEpoch 60: val_loss did not improve from 0.01213\n12/12 [==============================] - 17s 1s/step - loss: 0.0435 - accuracy: 0.9889 - val_loss: 0.0282 - val_accuracy: 0.9896\nEpoch 61/100\n12/12 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9945\nEpoch 61: val_loss did not improve from 0.01213\n12/12 [==============================] - 19s 2s/step - loss: 0.0202 - accuracy: 0.9945 - val_loss: 0.0997 - val_accuracy: 0.9479\nEpoch 62/100\n12/12 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9922\nEpoch 62: val_loss did not improve from 0.01213\n12/12 [==============================] - 17s 1s/step - loss: 0.0178 - accuracy: 0.9922 - val_loss: 0.0513 - val_accuracy: 0.9792\nEpoch 63/100\n12/12 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9972\nEpoch 63: val_loss did not improve from 0.01213\n12/12 [==============================] - 18s 1s/step - loss: 0.0066 - accuracy: 0.9972 - val_loss: 0.0357 - val_accuracy: 0.9896\nEpoch 64/100\n12/12 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9917\nEpoch 64: val_loss did not improve from 0.01213\n12/12 [==============================] - 17s 1s/step - loss: 0.0276 - accuracy: 0.9917 - val_loss: 0.0629 - val_accuracy: 0.9688\nEpoch 65/100\n12/12 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9972\nEpoch 65: val_loss did not improve from 0.01213\n12/12 [==============================] - 19s 2s/step - loss: 0.0225 - accuracy: 0.9972 - val_loss: 0.0610 - val_accuracy: 0.9792\nEpoch 66/100\n12/12 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9972\nEpoch 66: val_loss did not improve from 0.01213\n12/12 [==============================] - 17s 1s/step - loss: 0.0149 - accuracy: 0.9972 - val_loss: 0.0627 - val_accuracy: 0.9688\nEpoch 67/100\n12/12 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9945\nEpoch 67: val_loss did not improve from 0.01213\n12/12 [==============================] - 20s 2s/step - loss: 0.0132 - accuracy: 0.9945 - val_loss: 0.0576 - val_accuracy: 0.9792\nEpoch 68/100\n12/12 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9945\nEpoch 68: val_loss did not improve from 0.01213\n12/12 [==============================] - 19s 2s/step - loss: 0.0158 - accuracy: 0.9945 - val_loss: 0.0680 - val_accuracy: 0.9583\nEpoch 69/100\n12/12 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9945\nEpoch 69: val_loss did not improve from 0.01213\n12/12 [==============================] - 19s 2s/step - loss: 0.0113 - accuracy: 0.9945 - val_loss: 0.1123 - val_accuracy: 0.9479\nEpoch 70/100\n12/12 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9972\nEpoch 70: val_loss did not improve from 0.01213\n12/12 [==============================] - 18s 1s/step - loss: 0.0251 - accuracy: 0.9972 - val_loss: 0.0139 - val_accuracy: 1.0000\nEpoch 71/100\n12/12 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9948\nEpoch 71: val_loss improved from 0.01213 to 0.00493, saving model to ./fpseweights2.h5\n12/12 [==============================] - 20s 2s/step - loss: 0.0150 - accuracy: 0.9948 - val_loss: 0.0049 - val_accuracy: 1.0000\nEpoch 72/100\n12/12 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.9945\nEpoch 72: val_loss did not improve from 0.00493\n12/12 [==============================] - 18s 2s/step - loss: 0.0085 - accuracy: 0.9945 - val_loss: 0.0286 - val_accuracy: 0.9792\nEpoch 73/100\n12/12 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9945\nEpoch 73: val_loss did not improve from 0.00493\n12/12 [==============================] - 19s 2s/step - loss: 0.0185 - accuracy: 0.9945 - val_loss: 0.0236 - val_accuracy: 0.9896\nEpoch 74/100\n12/12 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9945\nEpoch 74: val_loss did not improve from 0.00493\n12/12 [==============================] - 17s 1s/step - loss: 0.0142 - accuracy: 0.9945 - val_loss: 0.0605 - val_accuracy: 0.9792\nEpoch 75/100\n12/12 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9917\nEpoch 75: val_loss did not improve from 0.00493\n12/12 [==============================] - 20s 2s/step - loss: 0.0256 - accuracy: 0.9917 - val_loss: 0.0686 - val_accuracy: 0.9583\nEpoch 76/100\n12/12 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9945\nEpoch 76: val_loss did not improve from 0.00493\n12/12 [==============================] - 19s 2s/step - loss: 0.0237 - accuracy: 0.9945 - val_loss: 0.0262 - val_accuracy: 0.9792\nEpoch 77/100\n12/12 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9896\nEpoch 77: val_loss did not improve from 0.00493\n12/12 [==============================] - 18s 2s/step - loss: 0.0257 - accuracy: 0.9896 - val_loss: 0.1865 - val_accuracy: 0.9583\nEpoch 78/100\n12/12 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9778\nEpoch 78: val_loss did not improve from 0.00493\n12/12 [==============================] - 18s 1s/step - loss: 0.0535 - accuracy: 0.9778 - val_loss: 0.0602 - val_accuracy: 0.9792\nEpoch 79/100\n12/12 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9945\nEpoch 79: val_loss did not improve from 0.00493\n12/12 [==============================] - 17s 1s/step - loss: 0.0268 - accuracy: 0.9945 - val_loss: 0.2477 - val_accuracy: 0.9167\nEpoch 80/100\n12/12 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9917\nEpoch 80: val_loss did not improve from 0.00493\n12/12 [==============================] - 20s 2s/step - loss: 0.0259 - accuracy: 0.9917 - val_loss: 0.0723 - val_accuracy: 0.9583\nEpoch 81/100\n12/12 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9917\nEpoch 81: val_loss did not improve from 0.00493\n12/12 [==============================] - 19s 2s/step - loss: 0.0213 - accuracy: 0.9917 - val_loss: 0.0606 - val_accuracy: 0.9688\nEpoch 82/100\n12/12 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9972\nEpoch 82: val_loss did not improve from 0.00493\n12/12 [==============================] - 20s 2s/step - loss: 0.0101 - accuracy: 0.9972 - val_loss: 0.0445 - val_accuracy: 0.9688\nEpoch 83/100\n12/12 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9945\nEpoch 83: val_loss did not improve from 0.00493\n12/12 [==============================] - 18s 2s/step - loss: 0.0155 - accuracy: 0.9945 - val_loss: 0.0119 - val_accuracy: 1.0000\nEpoch 84/100\n12/12 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\nEpoch 84: val_loss did not improve from 0.00493\n12/12 [==============================] - 17s 1s/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0398 - val_accuracy: 0.9792\nEpoch 85/100\n12/12 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9889\nEpoch 85: val_loss did not improve from 0.00493\n12/12 [==============================] - 17s 1s/step - loss: 0.0145 - accuracy: 0.9889 - val_loss: 0.0514 - val_accuracy: 0.9792\nEpoch 86/100\n12/12 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.9972\nEpoch 86: val_loss did not improve from 0.00493\n12/12 [==============================] - 18s 1s/step - loss: 0.0085 - accuracy: 0.9972 - val_loss: 0.0462 - val_accuracy: 0.9792\nEpoch 87/100\n12/12 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9917\nEpoch 87: val_loss did not improve from 0.00493\n12/12 [==============================] - 18s 1s/step - loss: 0.0271 - accuracy: 0.9917 - val_loss: 0.0852 - val_accuracy: 0.9479\nEpoch 88/100\n12/12 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9972\nEpoch 88: val_loss did not improve from 0.00493\n12/12 [==============================] - 20s 2s/step - loss: 0.0135 - accuracy: 0.9972 - val_loss: 0.0504 - val_accuracy: 0.9896\nEpoch 89/100\n12/12 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9861\nEpoch 89: val_loss did not improve from 0.00493\n12/12 [==============================] - 17s 1s/step - loss: 0.0373 - accuracy: 0.9861 - val_loss: 0.3047 - val_accuracy: 0.9271\nEpoch 90/100\n12/12 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9834\nEpoch 90: val_loss did not improve from 0.00493\n12/12 [==============================] - 20s 2s/step - loss: 0.0482 - accuracy: 0.9834 - val_loss: 0.0900 - val_accuracy: 0.9792\nEpoch 91/100\n12/12 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9945\nEpoch 91: val_loss did not improve from 0.00493\n12/12 [==============================] - 17s 1s/step - loss: 0.0150 - accuracy: 0.9945 - val_loss: 0.1392 - val_accuracy: 0.9583\nEpoch 92/100\n12/12 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9945\nEpoch 92: val_loss did not improve from 0.00493\n12/12 [==============================] - 19s 2s/step - loss: 0.0244 - accuracy: 0.9945 - val_loss: 0.0051 - val_accuracy: 1.0000\nEpoch 93/100\n12/12 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9917\nEpoch 93: val_loss did not improve from 0.00493\n12/12 [==============================] - 18s 2s/step - loss: 0.0124 - accuracy: 0.9917 - val_loss: 0.0896 - val_accuracy: 0.9688\nEpoch 94/100\n12/12 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 0.9972\nEpoch 94: val_loss did not improve from 0.00493\n12/12 [==============================] - 19s 2s/step - loss: 0.0107 - accuracy: 0.9972 - val_loss: 0.0537 - val_accuracy: 0.9688\nEpoch 95/100\n12/12 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9917\nEpoch 95: val_loss did not improve from 0.00493\n12/12 [==============================] - 20s 2s/step - loss: 0.0279 - accuracy: 0.9917 - val_loss: 0.0727 - val_accuracy: 0.9583\nEpoch 96/100\n12/12 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9922\nEpoch 96: val_loss did not improve from 0.00493\n12/12 [==============================] - 17s 1s/step - loss: 0.0176 - accuracy: 0.9922 - val_loss: 0.0638 - val_accuracy: 0.9688\nEpoch 97/100\n12/12 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9945\nEpoch 97: val_loss did not improve from 0.00493\n12/12 [==============================] - 19s 2s/step - loss: 0.0145 - accuracy: 0.9945 - val_loss: 0.1113 - val_accuracy: 0.9583\nEpoch 98/100\n12/12 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 0.9945\nEpoch 98: val_loss did not improve from 0.00493\n12/12 [==============================] - 18s 1s/step - loss: 0.0076 - accuracy: 0.9945 - val_loss: 0.0330 - val_accuracy: 0.9896\nEpoch 99/100\n12/12 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.9972\nEpoch 99: val_loss did not improve from 0.00493\n12/12 [==============================] - 19s 2s/step - loss: 0.0092 - accuracy: 0.9972 - val_loss: 0.0626 - val_accuracy: 0.9792\nEpoch 100/100\n12/12 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\nEpoch 100: val_loss did not improve from 0.00493\n12/12 [==============================] - 18s 1s/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0456 - val_accuracy: 0.9688\n","output_type":"stream"}]},{"cell_type":"code","source":"# 6. Evaluate the model\nMobileNetEvaluationWA = MobileNetModelWA.evaluate(validation_generator)\nprint(\"Validation Accuracy: {:.2f}%\".format(MobileNetEvaluationWA[1] * 100))","metadata":{"execution":{"iopub.status.busy":"2023-10-11T10:30:47.644209Z","iopub.execute_input":"2023-10-11T10:30:47.644576Z","iopub.status.idle":"2023-10-11T10:30:50.844741Z","shell.execute_reply.started":"2023-10-11T10:30:47.644547Z","shell.execute_reply":"2023-10-11T10:30:50.843757Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"4/4 [==============================] - 2s 478ms/step - loss: 0.0881 - accuracy: 0.9796\nValidation Accuracy: 97.96%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Generate predictions for the test set\nMobileNetPredictionsWA = MobileNetModelWA.predict_generator(test_generator)\nMobileNetPredicted_labelsWA = np.argmax(MobileNetPredictionsWA, axis=1)\nMobileNetReportWA = classification_report(true_labels, MobileNetPredicted_labelsWA, target_names=class_names)\nprint(MobileNetReportWA)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T10:34:02.553512Z","iopub.execute_input":"2023-10-11T10:34:02.554290Z","iopub.status.idle":"2023-10-11T10:34:07.724264Z","shell.execute_reply.started":"2023-10-11T10:34:02.554251Z","shell.execute_reply":"2023-10-11T10:34:07.723320Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_70/52924266.py:2: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n  MobileNetPredictionsWA = MobileNetModelWA.predict_generator(test_generator)\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n    cataract       0.97      0.97      0.97        61\n      normal       0.97      0.97      0.97        60\n\n    accuracy                           0.97       121\n   macro avg       0.97      0.97      0.97       121\nweighted avg       0.97      0.97      0.97       121\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# 6. Evaluate the model\nMobileNetEvaluationWA = MobileNetModelWA.evaluate(validation_generator)\nprint(\"Validation Accuracy: {:.2f}%\".format(MobileNetEvaluationWA[1] * 100))","metadata":{"execution":{"iopub.status.busy":"2023-10-11T12:47:22.527846Z","iopub.execute_input":"2023-10-11T12:47:22.528193Z","iopub.status.idle":"2023-10-11T12:47:26.072063Z","shell.execute_reply.started":"2023-10-11T12:47:22.528167Z","shell.execute_reply":"2023-10-11T12:47:26.071027Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"4/4 [==============================] - 2s 476ms/step - loss: 0.0222 - accuracy: 0.9898\nValidation Accuracy: 98.98%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}